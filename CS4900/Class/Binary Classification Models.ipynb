{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae721ee",
   "metadata": {},
   "source": [
    "# Train a binary classification model using PyTorch\n",
    "\n",
    "We use the breast cancer wisconsin dataset to train and test this binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab875870-f105-41c5-9fc2-38370c357b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all the libraries here\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397c824-9fb2-4a3e-b451-92a867420cab",
   "metadata": {},
   "source": [
    "## Load breast cancer wisconsin dataset from scikit-learn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be13a314-a51c-4941-8651-1a8b4a337ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Classe names***********\n",
      "['malignant' 'benign']\n",
      "***********Feature names***********\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "***********Data sample festure values***********\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "***********Data size and feature size***********\n",
      "(569, 30)\n",
      "***********Data label values***********\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "#Classe names\n",
    "print(\"***********Classe names***********\")\n",
    "print(cancer.target_names)\n",
    "#Feature names\n",
    "print(\"***********Feature names***********\")\n",
    "print(cancer.feature_names)\n",
    "#Data sample festure values\n",
    "print(\"***********Data sample festure values***********\")\n",
    "print(cancer.data)\n",
    "print(\"***********Data size and feature size***********\")\n",
    "print(cancer.data.shape)\n",
    "#Data label values\n",
    "print(\"***********Data label values***********\")\n",
    "print(cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12167c",
   "metadata": {},
   "source": [
    "## Convert the data to PyTorch tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601fef4b-badd-4fe6-a456-2ccafffaaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "#Split the data into two sets: 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create a TensorDataset for training and testing, respectively\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders: here, we use mini-batch gradient descent, so need to specify the batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e78bc",
   "metadata": {},
   "source": [
    "## Construct a binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31221457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        #The first \"30\" specifies that the feature dimension is 30, and the second \"1\" specifies that this is binary classification\n",
    "        self.fc = nn.Linear(30, 1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdff37a",
   "metadata": {},
   "source": [
    "## Set up some hyperparameters: use binary cross entropy loss, gradient descent with Adam optimizer, learning rate, and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f24ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification(\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "lossfunction = nn.BCEWithLogitsLoss() #Binary cross entropy loss for binary classification\n",
    "\n",
    "#Instantiate the model from \"BinaryClassification\" class definition\n",
    "model = BinaryClassification()\n",
    "\n",
    "#Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fee198",
   "metadata": {},
   "source": [
    "## Train the model using training data (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbfe9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1): Train accuracy: 0.6308, loss: 0.603\n",
      "epoch (2): Train accuracy: 0.7890, loss: 0.070\n",
      "epoch (3): Train accuracy: 0.8747, loss: 0.035\n",
      "epoch (4): Train accuracy: 0.8791, loss: 0.031\n",
      "epoch (5): Train accuracy: 0.8703, loss: 0.030\n",
      "epoch (6): Train accuracy: 0.8813, loss: 0.028\n",
      "epoch (7): Train accuracy: 0.8615, loss: 0.032\n",
      "epoch (8): Train accuracy: 0.8879, loss: 0.023\n",
      "epoch (9): Train accuracy: 0.8901, loss: 0.023\n",
      "epoch (10): Train accuracy: 0.8703, loss: 0.027\n",
      "epoch (11): Train accuracy: 0.8923, loss: 0.020\n",
      "epoch (12): Train accuracy: 0.8725, loss: 0.027\n",
      "epoch (13): Train accuracy: 0.8923, loss: 0.026\n",
      "epoch (14): Train accuracy: 0.9011, loss: 0.021\n",
      "epoch (15): Train accuracy: 0.8769, loss: 0.021\n",
      "epoch (16): Train accuracy: 0.9011, loss: 0.015\n",
      "epoch (17): Train accuracy: 0.8879, loss: 0.020\n",
      "epoch (18): Train accuracy: 0.8967, loss: 0.017\n",
      "epoch (19): Train accuracy: 0.8989, loss: 0.018\n",
      "epoch (20): Train accuracy: 0.9143, loss: 0.012\n",
      "epoch (21): Train accuracy: 0.9187, loss: 0.015\n",
      "epoch (22): Train accuracy: 0.9011, loss: 0.014\n",
      "epoch (23): Train accuracy: 0.8308, loss: 0.040\n",
      "epoch (24): Train accuracy: 0.9033, loss: 0.024\n",
      "epoch (25): Train accuracy: 0.9231, loss: 0.012\n",
      "epoch (26): Train accuracy: 0.8989, loss: 0.018\n",
      "epoch (27): Train accuracy: 0.8505, loss: 0.030\n",
      "epoch (28): Train accuracy: 0.9077, loss: 0.015\n",
      "epoch (29): Train accuracy: 0.8527, loss: 0.034\n",
      "epoch (30): Train accuracy: 0.9077, loss: 0.017\n",
      "epoch (31): Train accuracy: 0.9231, loss: 0.013\n",
      "epoch (32): Train accuracy: 0.8945, loss: 0.020\n",
      "epoch (33): Train accuracy: 0.8791, loss: 0.024\n",
      "epoch (34): Train accuracy: 0.9033, loss: 0.018\n",
      "epoch (35): Train accuracy: 0.9055, loss: 0.020\n",
      "epoch (36): Train accuracy: 0.9011, loss: 0.017\n",
      "epoch (37): Train accuracy: 0.8989, loss: 0.018\n",
      "epoch (38): Train accuracy: 0.9165, loss: 0.016\n",
      "epoch (39): Train accuracy: 0.9099, loss: 0.013\n",
      "epoch (40): Train accuracy: 0.8879, loss: 0.023\n",
      "epoch (41): Train accuracy: 0.9143, loss: 0.016\n",
      "epoch (42): Train accuracy: 0.9033, loss: 0.020\n",
      "epoch (43): Train accuracy: 0.8791, loss: 0.024\n",
      "epoch (44): Train accuracy: 0.8989, loss: 0.026\n",
      "epoch (45): Train accuracy: 0.9099, loss: 0.023\n",
      "epoch (46): Train accuracy: 0.9319, loss: 0.013\n",
      "epoch (47): Train accuracy: 0.9319, loss: 0.011\n",
      "epoch (48): Train accuracy: 0.9011, loss: 0.019\n",
      "epoch (49): Train accuracy: 0.9209, loss: 0.014\n",
      "epoch (50): Train accuracy: 0.9275, loss: 0.011\n"
     ]
    }
   ],
   "source": [
    "#Define the training function\n",
    "def train(epoch, model, train_dataloader, optimizer, lossfunction):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_total, train_correct = 0.0, 0.0 \n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the predicted output\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = lossfunction(predictions, y_batch)\n",
    "        \n",
    "        #Update the weights usning gradient descent with Adam optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Convert probabilities to binary predictions (if probability >= 0.5 reutrn positive prediction)\n",
    "        train_predicted = (predictions >= 0.5).float()\n",
    "        \n",
    "        #Calculate the training statistics\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (train_predicted == y_batch).sum().item()\n",
    "\n",
    "    print(\"epoch (%d): Train accuracy: %.4f, loss: %.3f\" % (epoch, train_correct/train_total, train_loss/train_total))\n",
    "    \n",
    "    \n",
    "#Train the model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, model, train_dataloader, optimizer, lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bad2c",
   "metadata": {},
   "source": [
    "## Test the model using test data (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8fc9d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9737, macro f1_score: 0.9790\n"
     ]
    }
   ],
   "source": [
    "#Define the test function\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct, test_total = 0.0, 0.0\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            #Convert probabilities to binary predictions (if probability >= 0.5 reutrn positive prediction)\n",
    "            test_predicted = (predictions >= 0.5).float()\n",
    "        \n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (test_predicted == y_batch).sum().item()\n",
    "            \n",
    "            y_pred += test_predicted.tolist()\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print('Test accuracy: %.4f, macro f1_score: %.4f' % (test_correct / test_total, f1))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "#Test the model\n",
    "y_pred = test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d361d",
   "metadata": {},
   "source": [
    "## Output the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37c3ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[41  2]\n",
      " [ 1 70]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b09728",
   "metadata": {},
   "source": [
    "## Calculate accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "456a8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "F1-score: 0.98\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(acc))\n",
    "print('F1-score: {:.2f}'.format(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
