{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e7773d",
   "metadata": {},
   "source": [
    "### CEG 4900 Trustworthy Machine Learning - Lab Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fb92a",
   "metadata": {},
   "source": [
    "# <center>Poisoning Attacks and Backdoor Attacks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0a0d2",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "The learning objective of this lab assignment is for students to understand poisoning and backdoor attacks and the corresponding defenses against these attacks, including how to perform label flipping poisoning attack and hidden trigger backdoor attack, and how to defend against these attacks using data sanitization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633487e0",
   "metadata": {},
   "source": [
    "#### Lecture notes. \n",
    "Detailed coverage of these topics can be found in the following:\n",
    "<li>Poisoning Attacks</li>\n",
    "<li>Poisoning Defenses</li>\n",
    "<li>Backdoor Attacks</li>\n",
    "\n",
    "#### Code demonstrations.\n",
    "<li>Code-Label Flipping Attack.ipynb</li>\n",
    "<li>Code-Data Sanitization.ipynb</li>\n",
    "<li>Code-Hidden Trigger Backdoor Attack.ipynb</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d253216",
   "metadata": {},
   "source": [
    "### 2. Submission\n",
    "You need to submit a detailed lab report with code, running results, and answers to the questions. If you submit <font color='red'>a jupyter notebook (“Firstname-Lastname-4900-Lab2.ipynb”)</font>, please fill in this file directly and place the code, running results, and answers in order for each question. If you submit <font color='red'>a PDF report (“Firstname-Lastname-4900-Lab2.pdf”) with code file (“Firstname-Lastname-4900-Lab2.py”)</font>, please include the screenshots (code and running results) with answers for each question in the report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ac85",
   "metadata": {},
   "source": [
    "### 3. Questions (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143a65f",
   "metadata": {},
   "source": [
    "#### Preparations: import the required libraries and define functions\n",
    "\n",
    "Please run the following cell to import all the required libraries and define some necessary functions before complete the coding questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all the libraries here\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, ConcatDataset, Dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set the random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Define the training function for training model using train_dataloader\n",
    "def train(epoch, model, train_dataloader, optimizer, lossfunction):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_total, train_correct = 0.0, 0.0 \n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the predicted output\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = lossfunction(predictions, y_batch)\n",
    "        \n",
    "        #Update the weights usning gradient descent with Adam optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Convert probabilities to multi-class predictions (reutrn the class with the maximal proability)\n",
    "        _, train_predicted = torch.max(predictions.data, 1)\n",
    "        \n",
    "        #Calculate the training statistics\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (train_predicted == y_batch).sum().item()\n",
    "\n",
    "    print(\"epoch (%d): Train accuracy: %.4f, loss: %.3f\" % (epoch, train_correct/train_total, train_loss/train_total))\n",
    "\n",
    "#Define the test function for evaluating the trained model using test_dataloader\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct, test_total = 0.0, 0.0\n",
    "    y_test, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            _, test_predicted = torch.max(predictions.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (test_predicted == y_batch).sum().item()\n",
    "            \n",
    "            y_test += y_batch.tolist()\n",
    "            y_pred += test_predicted.tolist()\n",
    "\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print('Test accuracy: %.4f, macro f1_score: %.4f' % (test_correct / test_total, macro_f1))\n",
    "\n",
    "#Define the function that returns a predicted label for a single input sample\n",
    "def predict_label(model, single_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        prediction = model(single_input)\n",
    "        _, predicted_label = torch.max(prediction.data, 1)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "#Define the function that convert the raw dataset to PyTorch tensor\n",
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.base_dataset[index]\n",
    "        return x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "#Define the function that returns model parameters\n",
    "def weight_parameters(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        parameters = list(model.parameters())[0]\n",
    "    \n",
    "    return parameters.detach().squeeze()\n",
    "\n",
    "#Define the function that plots the given images\n",
    "def plot_digits(instances, labels, images_per_row=5):\n",
    "    for i in range(len(instances)):\n",
    "        idx = i // images_per_row\n",
    "        idy = i % images_per_row \n",
    "        ax[idx, idy].imshow(instances[i].squeeze())\n",
    "        ax[idx, idy].set_title(class_names[labels[i]])\n",
    "        ax[idx, idy].axis(\"off\")\n",
    "\n",
    "#Define the function to train a linear model and get y_hat for label flipping attack\n",
    "def train_linear_model(epoch, linear_model, linear_model_optimizer, X, y, lossfunction):\n",
    "    linear_model.train()\n",
    "    linear_model_optimizer.zero_grad()\n",
    "    linear_model_outputs = linear_model(X)\n",
    "    linear_model_loss = -lossfunction(linear_model_outputs, y) #Need to place \"-\" before loss function\n",
    "    linear_model_loss.backward()\n",
    "    linear_model_optimizer.step()\n",
    "\n",
    "    print('Epoch: {:d}'.format(epoch),\n",
    "          'linear_model_loss: {:.4f}'.format(linear_model_loss.item()))\n",
    "\n",
    "#Define the function to train a feature collision model for hidden trigger backdoor attack\n",
    "def train_feature_collision_model(i, epoch, feature_collision_model, feature_collision_model_optimizer, triggered_instance, base_instance):\n",
    "    feature_collision_model.train()\n",
    "    feature_collision_model_optimizer.zero_grad()\n",
    "    feature_collision_objective = feature_collision_model(triggered_instance, base_instance)\n",
    "    feature_collision_objective.backward()\n",
    "    feature_collision_model_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b48b09",
   "metadata": {},
   "source": [
    "## <font color='blue'>Label Flipping Attack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4353ca6",
   "metadata": {},
   "source": [
    "#### Load and preprocess the `Android dataset` data\n",
    "\n",
    "In Question 1, Question 2, Question 3, and Question 4, you will be using the `Breast Cancer Wisconsin dataset` to train a logistic regression model (binary classification model) to predict whether a tumor is `0: malignant` or `1: benign`, and perform a label flipping poisoning attack against this trained binary classification model. First, please run the following cell to load and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98706103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load breast cancer wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "#Assign features and labels to X and y\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "#Class names\n",
    "print(\"Class names: \", cancer.target_names)\n",
    "#Feature number\n",
    "print(\"Number of Features: \", X.shape[1])\n",
    "\n",
    "#Split the data into two sets: 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc9799",
   "metadata": {},
   "source": [
    "#### Question 1 (5 points):  \n",
    "\n",
    "**Implement function `answer_one( )` to train a binary model `binary_model` using `(X_train, y_train)` from `Breast Cancer Wisconsin dataset`, and evaluate the model performance by calculating accuracy and F1 score.** \n",
    "\n",
    "**To simplify the model training process, please directly use `LogisticRegression` from Scikit-learn library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6505fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    #Code here: build a logistic regression model using scikit-learn library LogisticRegression \n",
    "    binary_model = \n",
    "    #Code here: train binary_model using fit() function and (X_train, y_train) data \n",
    "    binary_model.\n",
    "    #Code here: use the trained model to predict on X_test\n",
    "    y_pred = \n",
    "    \n",
    "    #Code here: use y_test and y_pred to calculate accuracy and F1 using sklearn functions\n",
    "    accuracy = \n",
    "    f1 = \n",
    "    \n",
    "    return binary_model, accuracy, f1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "binary_model, clean_accuracy, clean_f1 = answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70db0b",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 1 (double click here to answer the questions in this cell):</font>  \n",
    "Before label flipping attack<br>\n",
    "The test accuracy is: ( ) <br>\n",
    "The test f1 score is: ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28237ee",
   "metadata": {},
   "source": [
    "#### Question 2 (10 points):  \n",
    "**Implement a label flipping attack to poison the labels of the training data `y_train`, such that the logistic regression model re-trained on the poisoned data would make misclassification and thus decrease its test accuracy.** \n",
    "\n",
    "**First, define `LinearClassificationNet` to construct a linear classifier as sarrogate model to facilitate $\\hat{y}$ optimization. After that, complete the function `answer_two( )` to use closed form solution to optimize lab flipping attack and obtain the flipping operation probabilities. The closed form of weight matrix is $W=(X^T\\cdot X)^{-1}X^T\\hat{y}$. Based on the weight matrix, the linear classifier can be fixed as $f_W(X) = X \\cdot W = X \\cdot (X^T\\cdot X)^{-1}X^T\\hat{y}$, where $\\hat{y}$ is the only parameter in the function. $\\hat{y}$ represents flipping operation probabilities to specify how possible a specified label should be flipped to 1.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2312b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_num = X_train.shape[0]\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Define the linear classifier clsss using closed form\n",
    "class LinearClassificationNet(nn.Module):\n",
    "    def __init__(self, train_num):\n",
    "        super(LinearClassificationNet, self).__init__()\n",
    "        #Code here: define self.y_hat as label flipping probability tensor with size of (train_num, 1)\n",
    "        self.y_hat = \n",
    "        #Code here: initialize self.y_hat as 0.5 (multiply 0.5 with the tensor)\n",
    "        self.y_hat = \n",
    "        #Code here: add self.y_hat to the model parameter (requires_grad needs to be True)\n",
    "        self.y_hat = \n",
    "\n",
    "    def closedform(self, x):\n",
    "        x_t = torch.transpose(x, 0, 1)\n",
    "        x_x = torch.mm(x_t, x)               #X^T.X\n",
    "        x_x_1 = torch.inverse(x_x)           #(X^T.X)^-1\n",
    "        x_x_1_t = torch.mm(x_x_1, x_t)       #(X^T.X)^-1.X^T\n",
    "        \n",
    "        return torch.mm(x_x_1_t, self.y_hat) #(X^T.X)^-1.X^T.y_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Code here: get the closed form\n",
    "        closedform = \n",
    "        #Code here: calculate the output of the linear classifier (f(X) = X.W = X.closedform)\n",
    "        y = \n",
    "        \n",
    "        return y\n",
    "    \n",
    "\n",
    "def answer_two():\n",
    "    #Code here: instantiate a linear model from LinearClassificationNet (need to pass a corresponding parameter)\n",
    "    linear_model = \n",
    "    \n",
    "    #Code here: specify the optimizer used for gradient descent for linear model training\n",
    "    linear_model_optimizer = \n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #Code here: call train_linear_model() function to maximize the loss i.e., miminize -loss (need to pass 6 parameters)\n",
    "        \n",
    "    \n",
    "    #Code here: use weight_parameters() to extract the model parameter and assign it to y_hat \n",
    "    y_hat = \n",
    "    #Code here: convert y_hat to the label flipping probabilities by normalizing it to [0, 1] using nn.Sigmoid()()\n",
    "    y_hat = \n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "y_hat = answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59824e93",
   "metadata": {},
   "source": [
    "#### Question 3 (8 points): \n",
    "\n",
    "**Implement the function `answer_three( )` for label flipping and attack performance evaluation. As $\\hat{y}$ specifies how possible a label should be flipped to 1, here you need to select the top training samples with the largest probabilities to perform label flipping. The maximum number of training samples you can poison is constrained as `epsilon`. After label flipping attack, please use the poisoned training data to retrain logistic regression model and evaluate its classification accuracy on the test data.**  \n",
    "\n",
    "**Set `epsilon=` using `30`, `60`, `90` respectively to evaluate how `epsilon` impacts on the attack performance.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602d7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epsilon: the number of labels to flip\n",
    "#Code here: set epsilon as 30, 60, and 90 respectively\n",
    "epsilon =     \n",
    "\n",
    "def answer_three(epsilon):\n",
    "    #Code here: extract indices whose probabilities are sorted in descending order by y_hat values\n",
    "    indices = \n",
    "    flipped_labels = y_train.copy()\n",
    "    \n",
    "    cnt = 0\n",
    "    #Code here: select the training samples with the largest probabilities and flip them to 1 \n",
    "    #(the number of flipped labels do not include the training labels whose original labels are 1)\n",
    "    for idx in indices:\n",
    "\n",
    "    \n",
    "    #Code here: create a new logistic regression model\n",
    "    poisoned_binary_model = \n",
    "    #Code here: use (X_train, flipped_labels) to train the model\n",
    "    poisoned_binary_model.\n",
    "    #Code here: predict on the test data X_test\n",
    "    y_pred_poisoned = \n",
    "    #Code here: calculate accuracy and f1_score between y_test and y_pred_poisoned\n",
    "    accuracy_poisoned = \n",
    "    f1_poisoned = \n",
    "    \n",
    "    return flipped_labels, accuracy_poisoned, f1_poisoned\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "flipped_labels, accuracy_poisoned, f1_poisoned = answer_three(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201f014",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 3 (double click here to answer the questions in this cell):</font> \n",
    "When epsilon = 30, the test accuracy is: ( ) and the f1 score is: ( ) <br>\n",
    "When epsilon = 60, the test accuracy is: ( ) and the f1 score is: ( ) <br>\n",
    "When epsilon = 90, the test accuracy is: ( ) and the f1 score is: ( ) <br>\n",
    "Compared to the test performance before attack, please describe your observation: ( ) <br>\n",
    "And summarize the impact of epsilon on the label flipping attack performance: ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270a5ba",
   "metadata": {},
   "source": [
    "## <font color='blue'>Data Sanitization against Label Flipping Attack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a746ea0",
   "metadata": {},
   "source": [
    "#### Question 4 (7 points):  \n",
    "**Implement the function `answer_four( )` to build a k-Nearest Neighbors (kNN) using clean data `(X_train, y_train)` to sanitize the poisoned training data `(X_train, flipped_labels)` and mitigate the effect of label flipping attack on the test data. After removing those potential poisoned training samples, all the remaining samples can be constructed as the new and sanitized training data for the new logistic regression model training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728947ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    #Code here: build a kNN model using scikit-learn library KNeighborsClassifier with 5 nearest neighbors\n",
    "    knn = \n",
    "    #Code here: train knn using fit() function and clean data (X_train, y_train) \n",
    "    knn.\n",
    "    \n",
    "    #Code here: use the kNN model to predict the labels for all training samples in X_train\n",
    "    y_pred_knn = \n",
    "    \n",
    "    #Code here: get the mask for all samples that are (flipped_labels == y_pred_knn) \n",
    "    mask = \n",
    "    #Code here: assigned the masked samples from (X_train, flipped_labels) to (X_sanitized, y_sanitized)  \n",
    "    X_sanitized = \n",
    "    y_sanitized = \n",
    "    \n",
    "    #Code here: create a new logistic regression model\n",
    "    sanitized_binary_model = \n",
    "    #Code here: use (X_sanitized, y_sanitized) to train the model\n",
    "    sanitized_binary_model.\n",
    "    #Code here: predict on the test data X_test\n",
    "    y_pred_sanitized = \n",
    "    #Code here: calculate accuracy and f1_score between y_test and y_pred_sanitized\n",
    "    accuracy_sanitized = \n",
    "    f1_sanitized = \n",
    "    \n",
    "    return accuracy_sanitized, f1_sanitized\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "accuracy_sanitized, f1_sanitized = answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d004b",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 4 (double click here to answer the questions in this cell):</font> \n",
    "After data sanitization, the test accuracy is: ( ), and the f1 score is: ( ) <br>\n",
    "Compared to the test performance in Question 3, please describe the difference: ( ), and briefly explain why data sanitization can be used to defend against poisoning attack: ( )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853a460",
   "metadata": {},
   "source": [
    "## <font color='blue'>Hidden Trigger Backdoor Attack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a769f5",
   "metadata": {},
   "source": [
    "#### Load and preprocess the `Fashion-MNIST` data\n",
    "\n",
    "In Question 5 and Question 6, you will be using the `Fashion-MNIST` to train a convolutional neural network model to predict the fashion product name of a given image, and perform backdoor attacks against this trained fashion product image classification model. First, download the `Fashion-MNIST` data directly from PyTorch and convert the dataset into Tensor used by PyTorch. \n",
    "\n",
    "Loading `Fashion-MNIST` data of 70,000 images may take some time. The downloaded `Fashion-MNIST` data file will be stored in  `data` folder under the same directory with your notebook/python file.\n",
    "\n",
    "The size of each image in `Fashion-MNIST` data is 28x28. Each image is fed as 28x28 matrix to convolutional neural network directly.\n",
    "\n",
    "**For simplicity, you will only use 500 images for training and 50 images for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19561043",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#Convert the dataset into Tensor used by PyTorch\n",
    "transform=transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "#Download the Fashion-MNIST data directly from PyTorch\n",
    "#The downloaded datasets are stored in data folder under the same folder with this jupyter notebook file\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create subset indices\n",
    "train_subset_size = 500\n",
    "test_subset_size = 50\n",
    "train_subset_indices = torch.randperm(len(train_dataset))[:train_subset_size]\n",
    "test_subset_indices = torch.randperm(len(test_dataset))[:test_subset_size]\n",
    "\n",
    "# Create subset dataset\n",
    "train_subset = Subset(train_dataset, train_subset_indices)\n",
    "test_subset = Subset(test_dataset, test_subset_indices)\n",
    "\n",
    "#Load the datasets into DataLoader\n",
    "train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#Plot some Fashion-MNIST examples\n",
    "dataiter = iter(train_dataloader)\n",
    "samples = next(dataiter)\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "example_images = samples[0][:10]\n",
    "example_labels = samples[1][:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a085026",
   "metadata": {},
   "source": [
    "#### Pre-train a convolutional neural network using `Fashion-MNIST` data\n",
    "\n",
    "In the following questions, you will focus on performing and evaluating hidden trigger backdoor attack; therefore, directly run the following unit to pre-trained a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6149c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-define the CNN clsss for image classification\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50) \n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = F.max_pool2d(x, 2)  \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#Set up some hyperparameters\n",
    "torch.manual_seed(2)\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.CrossEntropyLoss()\n",
    "clean_model = CNN()\n",
    "clean_optimizer = optim.Adam(clean_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#Train the clean model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, clean_model, train_dataloader, clean_optimizer, lossfunction)\n",
    "\n",
    "#Test the clean model\n",
    "test(clean_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec8248",
   "metadata": {},
   "source": [
    "#### Construct base instances (base label c) and target instances (across all labels that are different from c) for backdoor attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a base label: all the images with trojan trigger should be classified as this label\n",
    "base_label_name = 'Trouser'\n",
    "base_label_index = class_names.index(base_label_name)\n",
    "\n",
    "#Obtain all the target instances and base instances\n",
    "target_instances = []  #Instances with target_label (target_label is the label that is different from base_label) \n",
    "target_labels = []\n",
    "base_instances = []    #Instances with base_label \n",
    "base_labels = []\n",
    "\n",
    "for samples, labels in train_subset:\n",
    "    if labels == base_label_index:\n",
    "        base_instances.append(samples)\n",
    "        base_labels.append(labels)\n",
    "    if labels != base_label_index:\n",
    "        target_instances.append(samples)\n",
    "        target_labels.append(labels)\n",
    "\n",
    "target_instances = torch.stack(target_instances)\n",
    "base_instances = torch.stack(base_instances)\n",
    "\n",
    "#Plot some base instances\n",
    "print(\"-------Base instances-------\")\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "example_images = base_instances[:10]\n",
    "example_labels = base_labels[:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()\n",
    "\n",
    "print(\"-------Target instances-------\")\n",
    "#Plot some target instances\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "example_images = target_instances[:10]\n",
    "example_labels = target_labels[:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c978095",
   "metadata": {},
   "source": [
    "#### Question 5 (10 points):  \n",
    "**Please implement a hidden trigger backdoor attack that first adds trojan trigger patten to the target images, and then perturbs the base images toward the target images with trojan trigger to generate the perturbed images, where these perturbed images are the final poisoned images. Afterwards, injects these poisoned images into the training data `train_subset`, such that the CNN model re-trained on the poisoned data would correctly classify the test images without trojan trigger, but misclassify the test images with trojan trigger as the base label `base_label_name`.**\n",
    "\n",
    "**First, define `FeatureCollisionNet` to construct a feature collision class to facilitate the optimization of the perturbed images. Then, implement the function `answer_five( )` to add trojan trigger to the target images, and use feature collision to generate the perturbed images. Feature collision can be formalized as an optimization problem $x = min \\|f(x) - f(x_t)\\|^2_2 + \\beta\\|x - x_c)\\|^2_2$, where $f(\\cdot)$ is a neural network used to learn the task-specific features of an input image, $x_t$ is a target image, and $x_c$ is a base image. You can use the clean convoutional neural network `clean_model` pre-trained in the previous unit as the neural network to learn the task-specific features. By minimizing $\\|f(x) - f(x_t)\\|^2_2 + \\beta\\|x - x_c)\\|^2_2$, the optimal $x$ can be extracted as the perturbed images.**  \n",
    "\n",
    "**Set `beta=` using `1`, `5`, respectively to evaluate how `beta` impacts on the feature collision from base instance and target instance.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bf240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 50              \n",
    "learning_rate = 0.1\n",
    "poisoned_account = 50\n",
    "\n",
    "trojan_trigger = torch.Tensor([[1.]]).repeat(14, 8)\n",
    "\n",
    "#Code here: set beta as 1, 5 respectively\n",
    "beta = \n",
    "\n",
    "#Define the feature collision clsss to obtain the objective function\n",
    "class FeatureCollisionNet(nn.Module):\n",
    "    def __init__(self, target_instance, base_instance, beta):\n",
    "        super(FeatureCollisionNet, self).__init__()\n",
    "        #Code here: define self.x and initialize it as a base_instance\n",
    "        self.x = \n",
    "        #Code here: add self.x to the model parameter (requires_grad needs to be True)\n",
    "        self.x = \n",
    "\n",
    "        self.beta = beta                               \n",
    "        \n",
    "        self.pretrained_model = clean_model \n",
    "        self.pretrained_model.eval()  \n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False \n",
    "\n",
    "    def get_task_specific_features(self, input_instance):\n",
    "        output = self.pretrained_model(input_instance)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, target_instance, base_instance):\n",
    "        x = torch.clamp(self.x, 0, 1)\n",
    "        \n",
    "        #Code here: extract the task-soefici features for x\n",
    "        task_specific_x = \n",
    "        #Code here: extract the task-soefici features for target_instance\n",
    "        task_specific_target = \n",
    "        \n",
    "        #Feature collision is to minimize an objective function: ||f(x) - f(x_t)||^2 + beta||x - x_c)||^2\n",
    "        #Code here: construct the objective function for feature collision\n",
    "        objective = \n",
    "        \n",
    "        return objective\n",
    "    \n",
    "\n",
    "def answer_five(beta):       \n",
    "    triggered_instances = target_instances.clone()\n",
    "    triggered_instances[:, 0, :14, :8] = trojan_trigger\n",
    "    \n",
    "    poisoned_instances = []\n",
    "    \n",
    "    for i in range(poisoned_account):\n",
    "        #Code here: randomly select one triggered instance from triggered_instances\n",
    "        triggered_instance = \n",
    "        #Code here: randomly select one base instance from base_instances\n",
    "        base_instance = \n",
    "        \n",
    "        parameter_to_update = []\n",
    "        \n",
    "        #Code here: instantiate a feature collision model from FeatureCollisionNet (need to pass three parameters)\n",
    "        feature_collision_model = \n",
    "        \n",
    "        for p in feature_collision_model.parameters():\n",
    "            if p.requires_grad:\n",
    "                parameter_to_update.append(p)\n",
    "                break\n",
    "        \n",
    "        #Code here: specify the optimizer used for gradient descent for feature collision model training\n",
    "        #Note: parameter_to_update is the model parameter tensor that needs to be passed to the optimizer  \n",
    "        feature_collision_model_optimizer = \n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            #Code here: call train_feature_collision_model() function (need to pass 6 parameters)\n",
    "            \n",
    "        \n",
    "        poisoned_instances.append(torch.clamp(parameter_to_update[0].squeeze(0), 0, 1))\n",
    "    \n",
    "    poisoned_instances = torch.stack(poisoned_instances)\n",
    "    \n",
    "    return poisoned_instances.detach()\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "poisoned_instances = answer_five(beta)\n",
    "\n",
    "#Plot some poisoned instances: the instances should look like the base instance (Trouser) \n",
    "#but their task-specific features are closer to the target instances with trigger (tend to be predicted as target labels)\n",
    "poisoned_predicted = predict_label(clean_model, poisoned_instances)\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "example_images = poisoned_instances[:10]\n",
    "example_labels = poisoned_predicted[:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4d7ca",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 5 (double click here to answer the questions in this cell):</font> \n",
    "Hidden trigger backdoor attack is to perturb the base images towards the target images with trojan trigger, where the perturbed images look like the base instances but get closer to task-specific features of the target instances with trojan trigger. In this question, the base label is \"Trousers\". Based on the perturbed images plotted above, please submmarize how `beta` with different values (`1` and `5`) impacts on the looks of the perturbed images and their relationships with base images and target images: ( )\n",
    "\n",
    "Based on the perturbed images plotted above, you cannot spot the trojan trigger pattern. Please explain why: ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe164fa2",
   "metadata": {},
   "source": [
    "#### Question 6 (10 points): \n",
    "\n",
    "**Implement the function `answer_six( )` to add the poisoned images to the training data and retrain the CNN model to evaluate the hidden trigger backdoor attack performance, including the accuracy on the test images without trojan trigger and the accuracy on the test images with trojan trigger.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.CrossEntropyLoss()\n",
    "\n",
    "def answer_six():\n",
    "    poisoned_labels = torch.full((poisoned_account,), base_label_index, dtype=torch.long)\n",
    "    \n",
    "    #Code here: construct poisoned training dataset using (poisoned_instances, poisoned_labels)\n",
    "    poisoned_dataset = \n",
    "    \n",
    "    train_subset_wrapped = WrappedDataset(train_subset)\n",
    "    \n",
    "    #Code here: concatenate train_subset_wrapped and poisoned_dataset to poisoned_train_dataset \n",
    "    poisoned_train_dataset =  \n",
    "    #Code here: load poisoned_train_dataset into DataLoader\n",
    "    poisoned_train_dataloader = \n",
    "    \n",
    "    #Code here: instantialize a backdoor model from the defined CNN class\n",
    "    backdoor_model =     \n",
    "    #Code here: specify the optimizer used for gradient descent for backdoor model training\n",
    "    backdoor_optimizer = \n",
    "    \n",
    "    print(\"-------Training a backdoor model using the poisoned images-------\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #Code here: call train() function (need to pass five parameters)\n",
    "        #Note: the model, optimizer, and dataloader should be backdoor model/optimizer and poisoned_train_dataloader\n",
    "        \n",
    "    \n",
    "    print(\"-------Test the backdoor_model on the clean test data-------\")\n",
    "    #Code here: call test() function (need to pass two parameters)\n",
    "    #Note: the model should be backdoor model, and the clean test data should be test_dataloader\n",
    "    \n",
    "    \n",
    "    print(\"-------Test the backdoor_model on the poisoned test data with trojan trigger-------\")\n",
    "    test_instances = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for samples, labels in test_subset:\n",
    "        if labels != base_label_index:\n",
    "            backdoor_test_instances = samples.clone()\n",
    "            backdoor_test_instances[0, :14, :8] = trojan_trigger\n",
    "            test_instances.append(backdoor_test_instances)\n",
    "            test_labels.append(labels)\n",
    "    \n",
    "    test_instances = torch.stack(test_instances)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    #Code here: construct backdoor test dataset using (test_instances, test_labels)\n",
    "    backdoor_test_dataset = \n",
    "    #Code here: load backdoor_test_dataset into DataLoader\n",
    "    backdoor_test_dataloader = \n",
    "    \n",
    "    #Code here: call test() function (need to pass two parameters)\n",
    "    #Note: the model should be backdoor model, and the test data should be backdoor_test_dataloader\n",
    "    \n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26474a67",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 6 (double click here to answer the questions in this cell):</font> \n",
    "When testing the backdoor model on the test data without trojan trigger, the test accuracy is: ( ); f1 score is: ( ) <br>\n",
    "When testing the backdoor model on the test data with trojan trigger, the test accuracy is: ( ); f1 score is: ( ) <br>\n",
    "Based on these test performances, please describe your observation on hidden trigger backdoor attack: ( ) <br>\n",
    "And also summarize the advantage and disadvantage of hidden trigger backdoor attack regarding the visibility of its generated poisoned instances and the attack performance: ( )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
