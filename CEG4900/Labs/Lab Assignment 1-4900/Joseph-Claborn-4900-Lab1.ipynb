{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e7773d",
   "metadata": {},
   "source": [
    "### CEG 4900 Trustworthy Machine Learning - Lab Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fb92a",
   "metadata": {},
   "source": [
    "# <center>Adversarial Attacks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0a0d2",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "The learning objective of this lab assignment is for students to understand adversarial attacks and defenses, including how to train a simple multi-class classification model (softmax regression) and a convolutional neural network for image classification, and how to perform adversarial attacks gainst these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633487e0",
   "metadata": {},
   "source": [
    "#### Lecture notes. \n",
    "Detailed coverage of these topics can be found in the following:\n",
    "<li>Machine Learning Basics</li>\n",
    "<li>Evaluation Metrics for Classification</li>\n",
    "<li>Simple Feature-Space Adversarial Attack</li>\n",
    "<li>Gradient-based Adversarial Attack</li>\n",
    "\n",
    "#### Code demonstrations.\n",
    "<li>Code-Simple Feature-Space Adversarial Attack.ipynb</li>\n",
    "<li>Code-Gradient-based Adversarial Attack (FGSM).ipynb</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d253216",
   "metadata": {},
   "source": [
    "### 2. Submission\n",
    "You need to submit a detailed lab report with code, running results, and answers to the questions. If you submit <font color='red'>a jupyter notebook (“Firstname-Lastname-4900-Lab1.ipynb”)</font>, please fill in this file directly and place the code, running results, and answers in order for each question. If you submit <font color='red'>a PDF report (“Firstname-Lastname-4900-Lab1.pdf”) with code file (“Firstname-Lastname-4900-Lab1.py”)</font>, please include the screenshots (code and running results) with answers for each question in the report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ac85",
   "metadata": {},
   "source": [
    "### 3. Questions (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c0370",
   "metadata": {},
   "source": [
    "#### Preparations: import the required libraries and define functions\n",
    "\n",
    "Please run the following cell to import all the required libraries and define some necessary functions before complete the coding questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a857a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all the libraries here\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Define the training function\n",
    "def train(epoch, model, train_dataloader, optimizer, lossfunction):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_total, train_correct = 0.0, 0.0 \n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the predicted output\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = lossfunction(predictions, y_batch)\n",
    "        \n",
    "        #Update the weights usning gradient descent with Adam optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Convert probabilities to multi-class predictions (reutrn the class with the maximal proability)\n",
    "        _, train_predicted = torch.max(predictions.data, 1)\n",
    "        \n",
    "        #Calculate the training statistics\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (train_predicted == y_batch).sum().item()\n",
    "\n",
    "    print(\"epoch (%d): Train accuracy: %.4f, loss: %.3f\" % (epoch, train_correct/train_total, train_loss/train_total))\n",
    "\n",
    "#Define the test function for test_dataloader\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct, test_total = 0.0, 0.0\n",
    "    y_test, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            _, test_predicted = torch.max(predictions.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (test_predicted == y_batch).sum().item()\n",
    "            \n",
    "            y_test += y_batch.tolist()\n",
    "            y_pred += test_predicted.tolist()\n",
    "\n",
    "    print('Test accuracy: %.4f' % (test_correct / test_total))\n",
    "    \n",
    "    return y_test, y_pred\n",
    "\n",
    "#Define the function that returns a predicted label for single input sample\n",
    "def predict_label(model, single_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        prediction = model(single_input)\n",
    "        _, predicted_label = torch.max(prediction.data, 1)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "#Define the function that returns predicted probabilities for single input sample\n",
    "def predict_probabilities(model, single_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        prediction = model(single_input)\n",
    "        predicted_probabilities = torch.softmax(prediction, dim=1).squeeze(0)\n",
    "    \n",
    "    return predicted_probabilities\n",
    "\n",
    "#Define the function that returns model weight vector that is used to predict the target_label\n",
    "def weight_vector(model, target_label):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights = list(model.parameters())[0]\n",
    "    \n",
    "    return weights[target_label]\n",
    "\n",
    "#Define the function that plots the given images\n",
    "def plot_digits(instances, labels, images_per_row=5):\n",
    "    for i in range(len(instances)):\n",
    "        idx = i // images_per_row\n",
    "        idy = i % images_per_row \n",
    "        ax[idx, idy].imshow(instances[i].squeeze(), cmap=\"gray\")\n",
    "        ax[idx, idy].set_title(class_names[labels[i]])\n",
    "        ax[idx, idy].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b48b09",
   "metadata": {},
   "source": [
    "## <font color='blue'>Simple Feature-space Adversarial Attack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4353ca6",
   "metadata": {},
   "source": [
    "#### Load and preprocess the `Wine Recognition` data\n",
    "\n",
    "In Question 1, Question 2, Question 3, and Question 4, you will be using the `Wine Recognition Dataset` to train a simple multi-class classification (softmax regression) model to predict the `class` of a given wine (`0: class_0, 1: class_1, 2: class_2`), and perform a feature-space adversarial attack against this trained classification model. First, please run the following cell to load the dataset and convert the data to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98706103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names:  ['class_0' 'class_1' 'class_2']\n",
      "Feature names:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Number of Features:  13\n"
     ]
    }
   ],
   "source": [
    "#Load wine recognition dataset\n",
    "wine = load_wine()\n",
    "\n",
    "#Assign features and labels to X and y\n",
    "X, y = wine.data, wine.target\n",
    "#Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#Class names\n",
    "print(\"Class names: \", wine.target_names)\n",
    "#Feature names\n",
    "print(\"Feature names: \", wine.feature_names)\n",
    "#Feature number\n",
    "print(\"Number of Features: \", X.shape[1])\n",
    "\n",
    "#Split the data into two sets: 75% for training and 25% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset for training and testing, respectively\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders: here, we use mini-batch gradient descent, so need to specify the batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28237ee",
   "metadata": {},
   "source": [
    "#### Question 1 (7 points):  \n",
    "**First, set up the hyperparameters, and then define a `MultiClassification` to construct a single-layer multi-class classification model (softmax regression). Note: the number of features is 13 and the number of classes is 3** \n",
    "\n",
    "**After that, implement function `answer_one( )` to instantiate a model from the defined `MultiClassification`, call the pre-defined function `train(epoch, model, train_dataloader, optimizer, lossfunction)` to train this model, and call the pre-defined function `test(model, test_dataloader)` to evaluate the trained model. Also, use the returned `y_test` and `y_pred` to calculate micro F1 score and macro F1 score.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2312b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1): Train accuracy: 0.3308, loss: 0.082\n",
      "epoch (2): Train accuracy: 0.3308, loss: 0.082\n",
      "epoch (3): Train accuracy: 0.3383, loss: 0.082\n",
      "epoch (4): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (5): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (6): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (7): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (8): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (9): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (10): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (11): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (12): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (13): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (14): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (15): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (16): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (17): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (18): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (19): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (20): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (21): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (22): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (23): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (24): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (25): Train accuracy: 0.3985, loss: 0.075\n",
      "epoch (26): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (27): Train accuracy: 0.3985, loss: 0.080\n",
      "epoch (28): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (29): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (30): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (31): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (32): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (33): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (34): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (35): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (36): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (37): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (38): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (39): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (40): Train accuracy: 0.3985, loss: 0.077\n",
      "Test accuracy: 0.4000\n",
      "epoch (1): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (2): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (3): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (4): Train accuracy: 0.3985, loss: 0.080\n",
      "epoch (5): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (6): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (7): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (8): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (9): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (10): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (11): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (12): Train accuracy: 0.3985, loss: 0.080\n",
      "epoch (13): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (14): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (15): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (16): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (17): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (18): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (19): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (20): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (21): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (22): Train accuracy: 0.3985, loss: 0.080\n",
      "epoch (23): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (24): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (25): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (26): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (27): Train accuracy: 0.3985, loss: 0.076\n",
      "epoch (28): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (29): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (30): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (31): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (32): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (33): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (34): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (35): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (36): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (37): Train accuracy: 0.3985, loss: 0.077\n",
      "epoch (38): Train accuracy: 0.3985, loss: 0.079\n",
      "epoch (39): Train accuracy: 0.3985, loss: 0.078\n",
      "epoch (40): Train accuracy: 0.3985, loss: 0.080\n",
      "Test accuracy: 0.4000\n",
      "(MultiClassification(\n",
      "  (fc): Linear(in_features=13, out_features=3, bias=False)\n",
      "), 0.19047619047619047, 0.4)\n"
     ]
    }
   ],
   "source": [
    "#Define a class MultiClassification to construct a single-layer multi-class classification model\n",
    "class MultiClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassification, self).__init__()\n",
    "        #Code here: define a fully-connected layer\n",
    "        #The input is the number of features and the output is the number of classes\n",
    "        #Set bias as False\n",
    "        self.fc = nn.Linear(in_features=13, out_features=3, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Code here: set up the forward calculation\n",
    "        y = torch.softmax(self.fc(x), dim=1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "#Set up the hyperparameters    \n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Code here: based on your computing resources, assign epochs as a number in the range of [30, 50]\n",
    "epochs = 40                      \n",
    "learning_rate = 0.01                 \n",
    "weight_decay = 5e-4                 \n",
    "lossfunction = nn.CrossEntropyLoss()\n",
    "\n",
    "def answer_one():\n",
    "    #Code here: instantiate a model from the defined MultiClassification  \n",
    "    model = MultiClassification()\n",
    "    \n",
    "    #Code here: specify the Adam optimizer used for mini-batch gradient descent for model training\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "    \n",
    "    #Train the model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #Code here: call train() function for model training: you need to pass the corresponding parameters to this function\n",
    "        train(epoch, model, train_dataloader, optimizer, lossfunction)\n",
    "    \n",
    "    #Test the model: \n",
    "    #Code here: call test() function to evaluate the trained model: you need to pass the corresponding parameters to this function\n",
    "    y_test, y_pred = test(model, test_dataloader)\n",
    "    \n",
    "    #Code here: use y_test and y_pred to calculate the macro F1 and micro F1 using sklearn function\n",
    "    macrof1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "    microf1 = f1_score(y_test, y_pred, average = 'micro')\n",
    "    \n",
    "    return model, macrof1, microf1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "model, macrof1, microf1 = answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750a8ae",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 1 (double click here to answer the questions in this cell):</font>  \n",
    "The number of epochs you used for softmax regression training is: ( 40 ) <br>\n",
    "The test accuracy is: ( 0.4000 ) <br>\n",
    "The test macro f1 score is: ( 0.1905 ) <br>\n",
    "The test micro f1 score is: ( 0.4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9290d7",
   "metadata": {},
   "source": [
    "#### Randomly select an test input from test_dataset to perturb and select a target label to attack\n",
    "\n",
    "Please run the following cell to select a random test input from test_dataset and a target label, which will be used for the following questions to perform simple feature-space adversarial attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "745c3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the test input:  38\n",
      "Test input feature vector:  tensor([1.2370e+01, 1.0700e+00, 2.1000e+00, 1.8500e+01, 8.8000e+01, 3.5200e+00,\n",
      "        3.7500e+00, 2.4000e-01, 1.9500e+00, 4.5000e+00, 1.0400e+00, 2.7700e+00,\n",
      "        6.6000e+02])\n",
      "Test input original label:  1 class_1\n",
      "Target label:  2 class_2\n"
     ]
    }
   ],
   "source": [
    "#Set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Number of test samples\n",
    "number_of_samples = len(test_dataset)\n",
    "#Get a random index from [0, number_of_samples)\n",
    "index = np.random.randint(number_of_samples)\n",
    "\n",
    "#Select the test input to perturb\n",
    "test_input = test_dataset[index][0]\n",
    "test_input_label = test_dataset[index][1]\n",
    "\n",
    "#Here, we perform targeted adversarial attack: change the original_label to the target_label\n",
    "original_label = test_input_label.item() #class_1\n",
    "target_label = 2 #class_2\n",
    "\n",
    "print(\"The index of the test input: \", index)\n",
    "print(\"Test input feature vector: \", test_input)\n",
    "print(\"Test input original label: \", original_label, wine.target_names[original_label])\n",
    "print(\"Target label: \", target_label, wine.target_names[target_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59824e93",
   "metadata": {},
   "source": [
    "#### Question 2 (7 points): \n",
    "\n",
    "**Implement the function `answer_two(k)` to search for a good instance to guide the simple feature-space adversarial attack, where you need to assign the value of `k` first to specify the number of top target samples close to decision boundary you would like to collect for good instance searching.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602d7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m good_instance\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#Run your function in the cell to return the results\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m good_instance \u001b[38;5;241m=\u001b[39m \u001b[43manswer_two\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(good_instance)\n",
      "Cell \u001b[1;32mIn[65], line 20\u001b[0m, in \u001b[0;36manswer_two\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     17\u001b[0m         target_samples\u001b[38;5;241m.\u001b[39mappend(sample)\n\u001b[0;32m     18\u001b[0m         target_probs\u001b[38;5;241m.\u001b[39mappend(predicted_proabilities)\n\u001b[1;32m---> 20\u001b[0m target_samples \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m target_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(target_probs)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#Code here: rank target samples by highest probability for the original label\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "#Code here: assign k as any integer in the range of [5,10]\n",
    "k = 7\n",
    "\n",
    "def answer_two(k):\n",
    "    #Obtain all the test samples with the target label\n",
    "    target_samples = [] #Test samples that are predicted as the target label\n",
    "    target_probs = []   #The prediction probabilities for each test sample\n",
    "    \n",
    "    for sample, true_label in test_dataset:\n",
    "        #Code here: using predict_label() to predict label for each sample (note: please use sample.unsqueeze(0) to ensure batch dimensition)\n",
    "        predicted_label = predict_label(model, sample.unsqueeze(0))\n",
    "        #Code here: using predict_probabilities() to predict probabilities for each sample (note: please use sample.unsqueeze(0) to ensure batch dimensition)\n",
    "        predicted_proabilities = predict_probabilities(model, sample.unsqueeze(0))\n",
    "\n",
    "        #Code here: append sample amd its proabilities to target_samples and target_probs if the predicted_label is target_label\n",
    "        if predicted_label == target_label:\n",
    "            target_samples.append(sample)\n",
    "            target_probs.append(predicted_proabilities)\n",
    "\n",
    "    target_samples = torch.stack(target_samples)\n",
    "    target_probs = torch.stack(target_probs)\n",
    "    \n",
    "    #Code here: rank target samples by highest probability for the original label\n",
    "    closest_to_boundary_indices = torch.argsort(torch.max(target_probs, dim=1).values)\n",
    "    \n",
    "    #Code here: find the indices of top k target samples that are closest to decision boundary\n",
    "    top_k_boundary_indices = closest_to_boundary_indices[:k]\n",
    "    \n",
    "    #Code here: calculate manhattan distance (L1 distance) between test_input and target_samples\n",
    "    distances = torch.sum(torch.abs(target_samples - test_input), dim=1)\n",
    "    \n",
    "    #Code here: find the indices of target samples that have shortest distance to the test_input \n",
    "    nearest_neighbors_indices = torch.argsort(distances)\n",
    "    \n",
    "    #The good instance is initialized as the nearest neighbor\n",
    "    good_instance = target_samples[nearest_neighbors_indices[0]]\n",
    "\n",
    "    #Code here: find a good instance: one of the test_input's nearest neighbors that is among top k target samples close to decision boundary\n",
    "    for i in nearest_neighbors_indices:\n",
    "        if i in top_k_boundary_indices:\n",
    "            good_instance = target_samples[i]\n",
    "            break\n",
    "        \n",
    "    return good_instance\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "good_instance = answer_two(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201f014",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 2 (double click here to answer the questions in this cell):</font>  \n",
    "The k you used to search for top target samples close to decision boundary is: ( 7 ) <br>\n",
    "The good instance you found for guidance is: ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa2cc8",
   "metadata": {},
   "source": [
    "#### Question 3 (5 points): \n",
    "\n",
    "**Implement the function `answer_three( )` to find feature indices in descending order of feature importance, such that we can use greedy search method to perturb these features from the most important one to the least important one. When you have a feature index, you can get its feature name using `wine.feature_names[feature_index]`. For example, if the feature index is 5, then its feature name is `wine.feature_names[5]`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4, 9, 11, 7, 6, 0, 8, 12, 10, 1, 2, 5, 3], 'magnesium', 'alcalinity_of_ash')\n"
     ]
    }
   ],
   "source": [
    "def answer_three():\n",
    "    #Code here: use weight_vector() to extract feature importances for the target label which can be quantified by the weight vector to predict the target label\n",
    "    featue_importances = weight_vector(model, target_label)\n",
    "\n",
    "    #Code here: find the indices of features from the most important to the least important \n",
    "    featue_importances_indices = torch.argsort(featue_importances, descending = True).tolist()\n",
    "    \n",
    "    #Code here: extract the most important feature (name)\n",
    "    most_important = wine.feature_names[featue_importances_indices[0]]\n",
    "    \n",
    "    #Code here: extract the least important feature (name)\n",
    "    least_important = wine.feature_names[featue_importances_indices[-1]]\n",
    "    \n",
    "    return featue_importances_indices, most_important, least_important\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "featue_importances_indices, most_important, least_important = answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871884e",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 3 (double click here to answer the questions in this cell):</font>  \n",
    "The feature indices in descending order of feature importance are: ( 4, 9, 11, 7, 6, 0, 8, 12, 10, 1, 2, 5, 3 ) <br>\n",
    "The most important feature is (fill in its feature name): ( magnesium ) <br>\n",
    "The least important feature is (fill in its feature name): ( alcalinity_of_ash )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a746ea0",
   "metadata": {},
   "source": [
    "#### Question 4 (8 points): \n",
    "\n",
    "**Implement the function `answer_four( )` to perturb the `test_input` by directly updating the value of a specific feature in the original test input to the value of that in the good instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728947ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adversarial_example, perturbation_size\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Run your function in the cell to return the results\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m adversarial_example, perturbation_size \u001b[38;5;241m=\u001b[39m answer_four(\u001b[43mgood_instance\u001b[49m, featue_importances_indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'good_instance' is not defined"
     ]
    }
   ],
   "source": [
    "adversarial_example = test_input.clone()\n",
    "\n",
    "def answer_four():\n",
    "    #Code here: perturb the features from the most important one to the least important one\n",
    "    for feature in featue_importances_indices:\n",
    "        adversarial_example[feature] = good_instance[feature]\n",
    "\n",
    "    #Code here: calculate the size of perturbation\n",
    "    perturbation_size = torch.sum(torch.abs(adversarial_example - test_input)).item()\n",
    "    \n",
    "    return adversarial_example, perturbation_size\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "adversarial_example, perturbation_size = answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d004b",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 4 (double click here to answer the questions in this cell):</font>  \n",
    "The adversarial example is: ( ) <br>\n",
    "The size of perturbation is: ( ) <br>\n",
    "Based on the previous steps, please summarize why we need to find a good instance and use feature importances to facilitate adversarial attack: ( Finding a good instance and using feature importance is useful because it helps us to guide adversarial attacks by honing in on the most impactful features for the attack which increases effectiveness while also minimizing the scale of perturbations. This allows for the attack to be more subtle and harder for a user to detect when it is occurring. ), and the advantage and disadvantage are: ( The advantage(s) are: Efficiency, Mininmal Perturbation, and Higher Success Rate. The disadvantage(s) are: Dependency on Model Explainability, Transferability Issues, and Computational Overhead. )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853a460",
   "metadata": {},
   "source": [
    "## <font color='blue'>Gradient-based Adversarial Attack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a769f5",
   "metadata": {},
   "source": [
    "#### Load and preprocess the `Fashion-MNIST` data\n",
    "\n",
    "In Question 5 and Question 6, you will be using the `Fashion-MNIST` to train a convolutional neural network model to predict the fashion product name of a given image, and perform a gradient-based adversarial attack against this trained fashion product image classification model. First, download the `Fashion-MNIST` data directly from PyTorch and convert the dataset into Tensor used by PyTorch. \n",
    "\n",
    "Loading `Fashion-MNIST` data of 70,000 images may take some time. The downloaded `Fashion-MNIST` data file will be stored in  `data` folder under the same directory with your notebook/python file.\n",
    "\n",
    "The size of each image in `Fashion-MNIST` data is 28x28. Each image is fed as 28x28 matrix to convolutional neural network directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19561043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert the dataset into Tensor used by PyTorch\n",
    "transform=transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "#Download the Fashion-MNIST data directly from PyTorch\n",
    "#The downloaded datasets are stored in data folder under the same folder with this jupyter notebook file\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#Load the datasets into DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#Plot some Fashion-MNIST examples\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "dataiter = iter(train_dataloader)\n",
    "samples = next(dataiter)\n",
    "example_images = samples[0][:10]\n",
    "example_labels = samples[1][:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c978095",
   "metadata": {},
   "source": [
    "#### Question 5 (7 points):  \n",
    "**First, set up the epochs (all the other hyperparameters are defined in Question 1), and then define a class `CNN` to construct all the layers in the convolutional neural network.** \n",
    "\n",
    "**After that, implement function `answer_five( )` to instantiate a CNN model from the defined `CNN`, call the pre-defined function `train(epoch, model, train_dataloader, optimizer, lossfunction)` to train the CNN model, and call the pre-defined function `test(model, test_dataloader)` to evaluate the trained CNN model. Also, use the returned `y_test` and `y_pred` to calculate micro F1 score and macro F1 score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bf240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1): Train accuracy: 0.8162, loss: 0.008\n",
      "epoch (2): Train accuracy: 0.8622, loss: 0.006\n",
      "epoch (3): Train accuracy: 0.8668, loss: 0.006\n",
      "epoch (4): Train accuracy: 0.8720, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.8732, loss: 0.005\n",
      "epoch (6): Train accuracy: 0.8739, loss: 0.005\n",
      "epoch (7): Train accuracy: 0.8761, loss: 0.005\n",
      "Test accuracy: 0.8703\n",
      "Macro F1:  0.8707016853789605\n",
      "Micro F1:  0.8703\n"
     ]
    }
   ],
   "source": [
    "#Set the random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Code here: based on your computing resources, assign epochs as a number in the range of [5, 10]  \n",
    "epochs = 7                \n",
    "\n",
    "#Code here: define a class CNN to construct all the layers in the convolutional neural network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)                                   #Convolution: 1 input channel to 10 channels with kernel_size 5\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)                                   #Convolution: 10 channels to 20 channels with kernel_size 5\n",
    "        self.fc1=nn.Linear(320, 50)                                   #Fully-connected layer: 320 neurons to 50 neurons\n",
    "        self.fc2=nn.Linear(50, 10)                                   #Fully-connected layer: 50 neurons to 10 neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))                                   #Use ReLU as activation function for the convolution layer \n",
    "        x = F.max_pool2d(x, 2)                                   #Apply max_pooling on the output of the convolution layer with size 2\n",
    "        x = F.relu(self.conv2(x))                                   #Use ReLU as activation function for the convolution layer\n",
    "        x = F.max_pool2d(x, 2)                                   #Apply max_pooling on the output of the convolution layer with size 2\n",
    "        x = x.view(x.size(0), -1)                                   #Flatten all channels to a single vector\n",
    "        x = F.relu(self.fc1(x))                                   #Use ReLU as activation function for the fully-connected layer\n",
    "        x = self.fc2(x)                                   #Obtain the final output with 10 neurons for 10 classes\n",
    "        return x\n",
    "\n",
    "def answer_five():    \n",
    "    #Code here: instantiate a CNN model from the defined CNN class  \n",
    "    model = CNN()\n",
    "    \n",
    "    #Code here: specify the optimizer used for mini-batch gradient descent for model training\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "    \n",
    "    #Train the model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #Code here: call train() function for model training: you need to pass the corresponding parameters to this function\n",
    "        train(epoch, model, train_dataloader, optimizer, lossfunction)\n",
    "    \n",
    "    #Test the model: \n",
    "    #Code here: call test() function to evaluate the trained model: you need to pass the corresponding parameters to this function\n",
    "    y_test, y_pred = test(model, test_dataloader)\n",
    "    \n",
    "    #Code here: use y_test and y_pred to calculate the macro F1 and micro F1 using sklearn function\n",
    "    macrof1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "    microf1 = f1_score(y_test, y_pred, average = 'micro')\n",
    "    \n",
    "    return model, macrof1, microf1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "model, macrof1, microf1 = answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4d7ca",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 5 (double click here to answer the questions in this cell):</font>  \n",
    "The number of epochs you used for CNN training is: ( 7 ) <br>\n",
    "The test accuracy is: ( 0.8703 ) <br>\n",
    "The test macro f1 score is: ( 0.8707 ) <br>\n",
    "The test micro f1 score is: ( 0.8703 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74087a8",
   "metadata": {},
   "source": [
    "#### Question 6 (8 points):\n",
    "**Based on the convolutional neural network `model` trained in Question 5, please implement a fast gradient sign method attack in function `answer_six(epsilon, image_input, true_label)`, and then based on your implementation, answer the corresponding question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38e6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six(epsilon, image_input, true_label):\n",
    "    #Set requires_grad attribute of image_input tensor as true, which will be used to get the gradient\n",
    "    image_input.requires_grad = True\n",
    "    \n",
    "    #Code here: pass the input image through the trained model to get the output\n",
    "    prediction = model(image_input)\n",
    "\n",
    "    #Code here: calculate the loss\n",
    "    loss = lossfunction(prediction, torch.tensor([true_label]))\n",
    "\n",
    "    #Code here: zero all existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    #Code here: calculate gradients of loss in backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #Code here: obtain the gradient regarding input image\n",
    "    image_input_grad = image_input.grad.data\n",
    "\n",
    "    #Perform FGSM attack\n",
    "    #Code here: obtain the sign of the gradient\n",
    "    sign_grad = image_input_grad.sign()\n",
    "\n",
    "    #Code here: generate the perturbed image by adding the perturbation with epsilon and sign of gradient to the input image\n",
    "    perturbed_image = image_input + epsilon * sign_grad\n",
    "\n",
    "    #Code here: add clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "    return perturbed_image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b673a0",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 6 (double click here to answer the questions in this cell):</font>  \n",
    "Based on the previous implementation, please summarize the steps of adversarial attack using fast gradient sign method: ( The steps of an adversarial attack using the fast gradient sign method are as follows: 1. Enable gradient tracking for the input image. 2. Pass the input image through the model used to get predictions. 3. Compute the loss between the prediction and the true label. 4. Perform backwards propagation to obtain the gradients with respect to the input. 5. Take the sign of the gradient to determine the direction of the maximum loss increase. 6. Generate the adversarial example by adding epsilon multiplied by the gradient sign to the input. 7. Clip the pixel values to maintain valid image ranges. ), and explain why FGSM uses the sign of the gradient instead of the raw gradient: ( FGSM uses the sign of the gradient instead of the raw gradient because when the sign is used, it ensures that the perturbation direction is consistent, FGSM can control the magnitude with the epsilon, and using the sign if more computationally beneficial when compared to just using the raw gradient. )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33203f79",
   "metadata": {},
   "source": [
    "#### Question 7 (8 points):\n",
    "**Implement function `answer_seven( )` to call the FGSM function implemented in `answer_six(epsilon, image_input, true_label)` to generate adversarial examples, and also evaluate how varying `epsilon` on the attack performance and the input images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d737bb16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m plt\u001b[38;5;241m.\u001b[39myticks([], [])\n\u001b[0;32m     43\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEps: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epsilons[i]), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m orig, adv, ex \u001b[38;5;241m=\u001b[39m true_labels[i], adv_labels[i], \u001b[43madversarial_examples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOri:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAdv: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(class_names[orig], class_names[adv]))\n\u001b[0;32m     46\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(ex, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#epsilons for the size of perturbation\n",
    "epsilons = [0, .05, .1, .15, .2, .25]\n",
    "\n",
    "#Choose a random test image as input\n",
    "#Set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Number of test samples\n",
    "number_of_images = len(test_dataset)\n",
    "#Get a random index from [0, number_of_samples)\n",
    "image_index = np.random.randint(number_of_samples, size=1)\n",
    "\n",
    "#Select the test input to perturb\n",
    "test_images = torch.stack([test_dataset[i][0] for i in image_index])\n",
    "test_imagelabels = torch.tensor([test_dataset[i][1] for i in image_index])\n",
    "\n",
    "def answer_seven():\n",
    "    adversarial_examples = []\n",
    "    true_labels = []\n",
    "    adv_labels = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        #Code here: call answer_six() to generate the perturbed image: you need to pass the corresponding parameters to this function\n",
    "        perturbed_image = answer_six(epsilon, test_images[0].unsqueeze(0), test_imagelabels[0].item())\n",
    "        #Code here:: use predict_label() to classify the perturbed image\n",
    "        adv_label = predict_label(model, perturbed_image)\n",
    "    \n",
    "        adversarial_examples.append(perturbed_image.detach().numpy())\n",
    "        true_labels.append(test_imagelabels[0].item())\n",
    "        adv_labels.append(adv_label)\n",
    "    \n",
    "    return adversarial_examples, true_labels, adv_labels\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "adversarial_examples, true_labels, adv_labels = answer_seven()\n",
    "\n",
    "#Plot adversarial example at each epsilon\n",
    "plt.figure(figsize=(12,20))\n",
    "for i in range(len(epsilons)):\n",
    "    plt.subplot(1, len(epsilons), i + 1)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "    orig, adv, ex = true_labels[i], adv_labels[i], adversarial_examples[i][0].squeeze(0)\n",
    "    plt.title(\"Ori:{} -> \\nAdv: {}\".format(class_names[orig], class_names[adv]))\n",
    "    plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a0abc",
   "metadata": {},
   "source": [
    "#### <font color='red'>Answer 7 (double click here to answer the questions in this cell):</font>  \n",
    "The original label name is: ( ) <br>\n",
    "After performing adversarial attack using fast gradient sign method, you get a successful adversarial example when the epsilon is: ( ) <br>\n",
    "The adversarial label name is: ( ) <br>\n",
    "Please summarize the impact of perturbations with different epsilons on the attack performance and the visual quality of the input image: ( The impact of perturbations with different epsilons on the attack performance and the visual quality of the input image is that when using a small epsilon, small perturbations that are not visible to the human eye but can still cause the model to misclassify an image, allowing for a subtle and effective attack. A moderate epsilon allows for more noticable perturbations with a higher likelihood of the model misclassifying an image, allowing for a more balanced stance on effectiveness and image disortion. A large epsilon allows for significant visual perturbations to occur, which will reduce the stealth of the attack, but almost gurantee that an image misclassification will occur. )."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
