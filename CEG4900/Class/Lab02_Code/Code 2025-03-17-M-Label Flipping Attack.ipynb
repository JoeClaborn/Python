{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016ff9fa",
   "metadata": {},
   "source": [
    "# Label flipping attack\n",
    "\n",
    "We use the iris dataset to train and test this binary classification model, and then perform the label flipping attack on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4fd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64b3c2",
   "metadata": {},
   "source": [
    "## Load iris dataset from scikit-learn and convert the data to PyTorch tensors\n",
    "\n",
    "To implement binary classification, consider “virginica” as the positive label (1 - virginica), and the other two “setosa” and “versicolor” as the negative label (0 – non-virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5cebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = (iris.target == 2).astype(int)  # if virginica, then 1; if setosa or versicolor, then 0\n",
    "\n",
    "#Split the data into two sets: 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a0f36",
   "metadata": {},
   "source": [
    "## Train a binary classification model\n",
    "\n",
    "Since we will focus on performing and evaluating label flipping attack, we simplify the model training, and directly use `LogisticRegression` imported from `sklearn.linear_model` to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b28da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0000, F1_score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "binary_model = LogisticRegression(solver=\"newton-cg\", random_state=42)\n",
    "binary_model.fit(X_train, y_train)\n",
    "y_pred = binary_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test accuracy: %.4f, F1_score: %.4f\" % (accuracy, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc543c16",
   "metadata": {},
   "source": [
    "**<font color='red'>Before label flipping attack, the original test accuracy is 1.0000 and f1 score is 1.000</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54628b6",
   "metadata": {},
   "source": [
    "## Poisoning Attacks with Label Flipping Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a3c9",
   "metadata": {},
   "source": [
    "#### Use closed form method\n",
    "In this classification example, we are doing binary classification, and we can use a linear classifier and mean squared loss to obtain weight matrix $W=(X^T\\cdot X)^{-1}X^T\\hat{y}$. Based on the weight matrix, the linear classifier can be fixed as $f_W(X) = X \\cdot W = X \\cdot (X^T\\cdot X)^{-1}X^T\\hat{y}$. After that, $\\hat{y}$ is the only parameter in the function. We can consider $\\hat{y}$ as a probability distribution to specify how possible a specified label should be flipped to 1. In this way, $\\hat{y}$ can be calculated using gradient descent method. The new linear classifier can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d828ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassificationNet(nn.Module):\n",
    "    def __init__(self, train_num):\n",
    "        super(LinearClassificationNet, self).__init__()\n",
    "        self.y_hat = torch.ones(train_num, 1, dtype=torch.float32) #Each training sample has a probability for label flipping \n",
    "        self.y_hat = 0.5 * self.y_hat                  #Initialize y_hat = 0.5\n",
    "        self.y_hat = Parameter(self.y_hat, requires_grad=True)\n",
    "\n",
    "    def closedform(self, x):\n",
    "        x_t = torch.transpose(x, 0, 1)\n",
    "        x_x = torch.mm(x_t, x)               #X^T.X\n",
    "        x_x_1 = torch.inverse(x_x)           #(X^T.X)^-1\n",
    "        x_x_1_t = torch.mm(x_x_1, x_t)       #(X^T.X)^-1.X^T\n",
    "        \n",
    "        return torch.mm(x_x_1_t, self.y_hat) #(X^T.X)^-1.X^T.y_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear model is implemented as matrix multiplication between X and W (f(X) = X.W)\n",
    "        #Here W is represented using closed form\n",
    "        closedform = self.closedform(x)\n",
    "        y = torch.mm(x, closedform)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc32c1",
   "metadata": {},
   "source": [
    "#### Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a5ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_num = X_train.shape[0]\n",
    "\n",
    "linear_model = LinearClassificationNet(train_num)\n",
    "linear_model_optimizer = optim.Adam(linear_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#Convert the data to tensors that can be used by Pytorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe516d",
   "metadata": {},
   "source": [
    "#### Maximize the loss to obtain $\\hat{y}$\n",
    "\n",
    "By default, gradient descent is used for loss minimization. We can convert maximizing `lossfunction` to minimizing `-lossfunction` using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e965274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 linear_model_loss: -0.7991\n",
      "Epoch: 2 linear_model_loss: -0.8027\n",
      "Epoch: 3 linear_model_loss: -0.8063\n",
      "Epoch: 4 linear_model_loss: -0.8099\n",
      "Epoch: 5 linear_model_loss: -0.8136\n",
      "Epoch: 6 linear_model_loss: -0.8173\n",
      "Epoch: 7 linear_model_loss: -0.8210\n",
      "Epoch: 8 linear_model_loss: -0.8247\n",
      "Epoch: 9 linear_model_loss: -0.8284\n",
      "Epoch: 10 linear_model_loss: -0.8322\n",
      "Epoch: 11 linear_model_loss: -0.8359\n",
      "Epoch: 12 linear_model_loss: -0.8397\n",
      "Epoch: 13 linear_model_loss: -0.8435\n",
      "Epoch: 14 linear_model_loss: -0.8474\n",
      "Epoch: 15 linear_model_loss: -0.8512\n",
      "Epoch: 16 linear_model_loss: -0.8551\n",
      "Epoch: 17 linear_model_loss: -0.8590\n",
      "Epoch: 18 linear_model_loss: -0.8629\n",
      "Epoch: 19 linear_model_loss: -0.8668\n",
      "Epoch: 20 linear_model_loss: -0.8708\n",
      "Epoch: 21 linear_model_loss: -0.8748\n",
      "Epoch: 22 linear_model_loss: -0.8788\n",
      "Epoch: 23 linear_model_loss: -0.8828\n",
      "Epoch: 24 linear_model_loss: -0.8868\n",
      "Epoch: 25 linear_model_loss: -0.8909\n",
      "Epoch: 26 linear_model_loss: -0.8950\n",
      "Epoch: 27 linear_model_loss: -0.8991\n",
      "Epoch: 28 linear_model_loss: -0.9032\n",
      "Epoch: 29 linear_model_loss: -0.9074\n",
      "Epoch: 30 linear_model_loss: -0.9115\n",
      "Epoch: 31 linear_model_loss: -0.9157\n",
      "Epoch: 32 linear_model_loss: -0.9199\n",
      "Epoch: 33 linear_model_loss: -0.9242\n",
      "Epoch: 34 linear_model_loss: -0.9285\n",
      "Epoch: 35 linear_model_loss: -0.9328\n",
      "Epoch: 36 linear_model_loss: -0.9371\n",
      "Epoch: 37 linear_model_loss: -0.9414\n",
      "Epoch: 38 linear_model_loss: -0.9458\n",
      "Epoch: 39 linear_model_loss: -0.9502\n",
      "Epoch: 40 linear_model_loss: -0.9546\n",
      "Epoch: 41 linear_model_loss: -0.9590\n",
      "Epoch: 42 linear_model_loss: -0.9635\n",
      "Epoch: 43 linear_model_loss: -0.9679\n",
      "Epoch: 44 linear_model_loss: -0.9724\n",
      "Epoch: 45 linear_model_loss: -0.9770\n",
      "Epoch: 46 linear_model_loss: -0.9815\n",
      "Epoch: 47 linear_model_loss: -0.9861\n",
      "Epoch: 48 linear_model_loss: -0.9907\n",
      "Epoch: 49 linear_model_loss: -0.9953\n",
      "Epoch: 50 linear_model_loss: -1.0000\n",
      "Epoch: 51 linear_model_loss: -1.0046\n",
      "Epoch: 52 linear_model_loss: -1.0093\n",
      "Epoch: 53 linear_model_loss: -1.0140\n",
      "Epoch: 54 linear_model_loss: -1.0188\n",
      "Epoch: 55 linear_model_loss: -1.0235\n",
      "Epoch: 56 linear_model_loss: -1.0283\n",
      "Epoch: 57 linear_model_loss: -1.0331\n",
      "Epoch: 58 linear_model_loss: -1.0380\n",
      "Epoch: 59 linear_model_loss: -1.0428\n",
      "Epoch: 60 linear_model_loss: -1.0477\n",
      "Epoch: 61 linear_model_loss: -1.0526\n",
      "Epoch: 62 linear_model_loss: -1.0575\n",
      "Epoch: 63 linear_model_loss: -1.0625\n",
      "Epoch: 64 linear_model_loss: -1.0674\n",
      "Epoch: 65 linear_model_loss: -1.0724\n",
      "Epoch: 66 linear_model_loss: -1.0774\n",
      "Epoch: 67 linear_model_loss: -1.0825\n",
      "Epoch: 68 linear_model_loss: -1.0875\n",
      "Epoch: 69 linear_model_loss: -1.0926\n",
      "Epoch: 70 linear_model_loss: -1.0977\n",
      "Epoch: 71 linear_model_loss: -1.1028\n",
      "Epoch: 72 linear_model_loss: -1.1080\n",
      "Epoch: 73 linear_model_loss: -1.1132\n",
      "Epoch: 74 linear_model_loss: -1.1184\n",
      "Epoch: 75 linear_model_loss: -1.1236\n",
      "Epoch: 76 linear_model_loss: -1.1288\n",
      "Epoch: 77 linear_model_loss: -1.1341\n",
      "Epoch: 78 linear_model_loss: -1.1393\n",
      "Epoch: 79 linear_model_loss: -1.1446\n",
      "Epoch: 80 linear_model_loss: -1.1499\n",
      "Epoch: 81 linear_model_loss: -1.1553\n",
      "Epoch: 82 linear_model_loss: -1.1606\n",
      "Epoch: 83 linear_model_loss: -1.1660\n",
      "Epoch: 84 linear_model_loss: -1.1714\n",
      "Epoch: 85 linear_model_loss: -1.1769\n",
      "Epoch: 86 linear_model_loss: -1.1823\n",
      "Epoch: 87 linear_model_loss: -1.1878\n",
      "Epoch: 88 linear_model_loss: -1.1932\n",
      "Epoch: 89 linear_model_loss: -1.1987\n",
      "Epoch: 90 linear_model_loss: -1.2043\n",
      "Epoch: 91 linear_model_loss: -1.2098\n",
      "Epoch: 92 linear_model_loss: -1.2154\n",
      "Epoch: 93 linear_model_loss: -1.2209\n",
      "Epoch: 94 linear_model_loss: -1.2265\n",
      "Epoch: 95 linear_model_loss: -1.2322\n",
      "Epoch: 96 linear_model_loss: -1.2378\n",
      "Epoch: 97 linear_model_loss: -1.2434\n",
      "Epoch: 98 linear_model_loss: -1.2491\n",
      "Epoch: 99 linear_model_loss: -1.2548\n",
      "Epoch: 100 linear_model_loss: -1.2605\n",
      "tensor([0.8223, 0.8222, 0.3648, 0.3124, 0.8234, 0.8196, 0.8233, 0.8276, 0.8282,\n",
      "        0.4021, 0.8244, 0.3559, 0.8185, 0.8431, 0.3861, 0.8230, 0.8379, 0.8283,\n",
      "        0.8236, 0.8213, 0.8202, 0.8281, 0.5792, 0.8238, 0.8250, 0.8209, 0.8211,\n",
      "        0.8236, 0.8221, 0.3787, 0.8228, 0.8248, 0.3626, 0.8215, 0.3585, 0.3648,\n",
      "        0.8299, 0.3650, 0.3550, 0.8247, 0.8206, 0.8245, 0.4066, 0.8235, 0.8234,\n",
      "        0.8268, 0.8179, 0.8242, 0.3560, 0.8234, 0.8165, 0.3552, 0.8174, 0.3579,\n",
      "        0.8108, 0.8375, 0.4947, 0.8214, 0.8235, 0.8231, 0.8334, 0.8290, 0.8235,\n",
      "        0.8219, 0.8228, 0.8292, 0.3556, 0.8204, 0.3655, 0.3577, 0.8229, 0.8244,\n",
      "        0.8246, 0.3823, 0.8259, 0.3549, 0.8223, 0.3603, 0.7847, 0.3611, 0.8260,\n",
      "        0.4560, 0.8217, 0.8249, 0.8276, 0.8277, 0.8230, 0.8102, 0.8352, 0.3612,\n",
      "        0.8197, 0.8227, 0.3555, 0.8312, 0.8231, 0.3567, 0.8244, 0.8258, 0.8327,\n",
      "        0.4159, 0.8197, 0.5616, 0.8293, 0.8256, 0.3583, 0.3645, 0.8247, 0.8233,\n",
      "        0.3619, 0.8226, 0.8257, 0.4083])\n"
     ]
    }
   ],
   "source": [
    "#Define the function for linear model training\n",
    "def train_linear_model(epoch, linear_model, linear_model_optimizer, X, y, lossfunction):\n",
    "    linear_model.train()\n",
    "    linear_model_optimizer.zero_grad()\n",
    "    linear_model_outputs = linear_model(X)\n",
    "    linear_model_loss = -lossfunction(linear_model_outputs, y) #Need to place \"-\" before loss function\n",
    "    linear_model_loss.backward()\n",
    "    linear_model_optimizer.step()\n",
    "\n",
    "    print('Epoch: {:d}'.format(epoch+1),\n",
    "          'linear_model_loss: {:.4f}'.format(linear_model_loss.item()))\n",
    "\n",
    "#Define the function that returns model parameters\n",
    "def weight_parameters(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        parameters = list(model.parameters())[0]\n",
    "    \n",
    "    return parameters.detach().squeeze()\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(epochs):\n",
    "    train_linear_model(epoch, linear_model, linear_model_optimizer, X_train_tensor, y_train_tensor, lossfunction)\n",
    "\n",
    "#Obtain y_hat\n",
    "#y_hat is the parameters optimized by maximizing the loss function  \n",
    "y_hat = weight_parameters(linear_model)\n",
    "#Convert the parameters to the label flipping probabilities by normalizing the parameters to [0, 1]\n",
    "y_hat = nn.Sigmoid()(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d4910",
   "metadata": {},
   "source": [
    "#### Based on the obtained $\\hat{y}$, select top training samples to perform label flipping\n",
    "\n",
    "As $\\hat{y}$ specifies how possible a sample's label should be flipped to 1, here we directly use these probabilities select top training samples to perform label flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb6f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of labels to flip: epsilon\n",
    "epsilon = 20\n",
    "cnt = 0\n",
    "\n",
    "#Return indices whose probabilities are sorted in descending order by values\n",
    "indices = torch.argsort(y_hat, descending=True)\n",
    "flipped_labels = y_train.copy()\n",
    "\n",
    "#Select the training samples with the largest probabilities and flip them to 1 \n",
    "for idx in indices:\n",
    "    if flipped_labels[idx] != 1 and y_hat[idx] > 0.5:\n",
    "        flipped_labels[idx] = 1\n",
    "        cnt += 1\n",
    "        if cnt == epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab175fe",
   "metadata": {},
   "source": [
    "## Use the flipped labels to create poisoned training data and retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135365d",
   "metadata": {},
   "source": [
    "#### Use the flipped labels to create poisoned training data and retrain the model\n",
    "\n",
    "The poisoned data is (X_train, flipped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801812d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7368, F1_score: 0.7059\n"
     ]
    }
   ],
   "source": [
    "poisoned_binary_model = LogisticRegression(solver=\"newton-cg\", random_state=42)\n",
    "poisoned_binary_model.fit(X_train, flipped_labels)\n",
    "y_pred_poisoned = poisoned_binary_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_poisoned)\n",
    "f1 = f1_score(y_test, y_pred_poisoned)\n",
    "\n",
    "print(\"Test accuracy: %.4f, F1_score: %.4f\" % (accuracy, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe909a",
   "metadata": {},
   "source": [
    "**<font color='red'>Before label flipping attack, the original test accuracy is 1.0000 and f1 score is 1.0000. After label flipping attack (flipped 20 training samples), the test accuracy is 0.7368 and f1 score is 0.7059. The classification performance has been significantly decreased</font>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
