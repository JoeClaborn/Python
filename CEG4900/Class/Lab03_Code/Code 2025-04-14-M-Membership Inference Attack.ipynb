{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7444aad2",
   "metadata": {},
   "source": [
    "# Membership inference attack\n",
    "\n",
    "We use the MNIST data to implement this attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4fd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, ConcatDataset, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64b3c2",
   "metadata": {},
   "source": [
    "## Load and process MNIST data for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f1b90",
   "metadata": {},
   "source": [
    "For the membership inference attack introduced in class, the first step is to synthesize a dataset for a shadow model (surrogate model). Here, we sample this dataset directly from the loaded data and focus on the second step that constructs a set of shadow models to obtain member and non-member confidence scores for attack model training.\n",
    "\n",
    "`dataset_for_shadow` is the data used for shadow model training\n",
    "\n",
    "`dataset_for_target` is the data used for target model training\n",
    "\n",
    "`dataset_for_shadow` and `dataset_for_target` should be disjoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5cebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize images and convert the dataset into Tensor used by PyTorch\n",
    "transform=transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "#Download the MNIST data directly from PyTorch\n",
    "dataset_for_shadow = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataset_for_target = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = ['0', '1', '2', '3', '4',\n",
    "               '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e83e7",
   "metadata": {},
   "source": [
    "## Prepare Datasets for target model and shadow models (surrogate models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ba0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#The number of shadow models\n",
    "num_shadow_models = 10\n",
    "\n",
    "#The size of samples for each shadow model\n",
    "shadow_size = 2500\n",
    "\n",
    "#List of training datasets: each training set is used to train one shadow model\n",
    "list_shadow_loader = []\n",
    "#List of Unseen datasets: one for each shadow model\n",
    "list_unseen_loader = []\n",
    "\n",
    "for _ in range(num_shadow_models):\n",
    "    #training data and unseen data should be disjoint\n",
    "    #Obtain the indices for training data and unseen data\n",
    "    total_indices = np.arange(len(dataset_for_shadow))\n",
    "    \n",
    "    train_indices = np.random.choice(total_indices, shadow_size, replace=False)\n",
    "    remaining_indices = np.setdiff1d(total_indices, train_indices)\n",
    "    unseen_indices = np.random.choice(remaining_indices, shadow_size, replace=False)\n",
    "    \n",
    "    subset_train = Subset(dataset_for_shadow, train_indices)\n",
    "    subset_unseen = Subset(dataset_for_shadow, unseen_indices)\n",
    "    \n",
    "    subset_train_loader = DataLoader(subset_train, batch_size=32, shuffle=True)\n",
    "    subset_unseen_loader = DataLoader(subset_unseen, batch_size=32, shuffle=False)\n",
    "    \n",
    "    list_shadow_loader.append(subset_train_loader)\n",
    "    list_unseen_loader.append(subset_unseen_loader)\n",
    "\n",
    "#Data preparation for target model\n",
    "target_train_loader = DataLoader(dataset_for_target, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3a0a0",
   "metadata": {},
   "source": [
    "## Train a target model\n",
    "\n",
    "We need to a target model, such that the attack can perform membership inference attack to determine if a record is used to train this target model or not\n",
    "\n",
    "#### Set up model definition, hyperparameters, and  the train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b28da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.CrossEntropyLoss()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50) \n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = F.max_pool2d(x, 2)   \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "target_model = CNN()\n",
    "target_optimizer = optim.Adam(target_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#Pre-define the training function\n",
    "def train(epoch, model, train_dataloader, optimizer, lossfunction):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_total, train_correct = 0.0, 0.0 \n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the predicted output\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = lossfunction(predictions, y_batch)\n",
    "        \n",
    "        #Update the weights usning gradient descent with Adam optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Convert probabilities to multi-class predictions (reutrn the class with the maximal proability)\n",
    "        _, train_predicted = torch.max(predictions.data, 1)\n",
    "        \n",
    "        #Calculate the training statistics\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (train_predicted == y_batch).sum().item()\n",
    "\n",
    "    print(\"epoch (%d): Train accuracy: %.4f, loss: %.3f\" % (epoch, train_correct/train_total, train_loss/train_total))\n",
    " \n",
    "\n",
    "#Pre-define the test function\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct, test_total = 0.0, 0.0\n",
    "    y_test, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            _, test_predicted = torch.max(predictions.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (test_predicted == y_batch).sum().item()\n",
    "            \n",
    "            y_test += y_batch.tolist()\n",
    "            y_pred += test_predicted.tolist()\n",
    "\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print('Test accuracy: %.4f, macro f1_score: %.4f' % (test_correct / test_total, macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6c8f4",
   "metadata": {},
   "source": [
    "#### Train the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1acf8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1): Train accuracy: 0.8971, loss: 0.010\n",
      "epoch (2): Train accuracy: 0.9640, loss: 0.004\n",
      "epoch (3): Train accuracy: 0.9736, loss: 0.003\n",
      "epoch (4): Train accuracy: 0.9745, loss: 0.002\n",
      "epoch (5): Train accuracy: 0.9780, loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "#Train the target model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, target_model, target_train_loader, target_optimizer, lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb36ec",
   "metadata": {},
   "source": [
    "## Train a set of shadow models\n",
    "\n",
    "####  Use `shadow_loader` to train a shadow model and create member dataset vs non-member dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1613db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Shadow model: 1-----------\n",
      "epoch (1): Train accuracy: 0.5224, loss: 0.041\n",
      "epoch (2): Train accuracy: 0.9048, loss: 0.009\n",
      "epoch (3): Train accuracy: 0.9312, loss: 0.007\n",
      "epoch (4): Train accuracy: 0.9564, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9488, loss: 0.005\n",
      "-----------Shadow model: 2-----------\n",
      "epoch (1): Train accuracy: 0.7088, loss: 0.028\n",
      "epoch (2): Train accuracy: 0.8996, loss: 0.009\n",
      "epoch (3): Train accuracy: 0.9356, loss: 0.006\n",
      "epoch (4): Train accuracy: 0.9484, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9500, loss: 0.005\n",
      "-----------Shadow model: 3-----------\n",
      "epoch (1): Train accuracy: 0.7168, loss: 0.028\n",
      "epoch (2): Train accuracy: 0.9196, loss: 0.008\n",
      "epoch (3): Train accuracy: 0.9372, loss: 0.006\n",
      "epoch (4): Train accuracy: 0.9520, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9644, loss: 0.003\n",
      "-----------Shadow model: 4-----------\n",
      "epoch (1): Train accuracy: 0.7232, loss: 0.025\n",
      "epoch (2): Train accuracy: 0.9296, loss: 0.007\n",
      "epoch (3): Train accuracy: 0.9500, loss: 0.006\n",
      "epoch (4): Train accuracy: 0.9472, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9760, loss: 0.002\n",
      "-----------Shadow model: 5-----------\n",
      "epoch (1): Train accuracy: 0.7616, loss: 0.023\n",
      "epoch (2): Train accuracy: 0.9376, loss: 0.006\n",
      "epoch (3): Train accuracy: 0.9532, loss: 0.004\n",
      "epoch (4): Train accuracy: 0.9608, loss: 0.004\n",
      "epoch (5): Train accuracy: 0.9628, loss: 0.003\n",
      "-----------Shadow model: 6-----------\n",
      "epoch (1): Train accuracy: 0.7376, loss: 0.025\n",
      "epoch (2): Train accuracy: 0.9428, loss: 0.006\n",
      "epoch (3): Train accuracy: 0.9560, loss: 0.004\n",
      "epoch (4): Train accuracy: 0.9648, loss: 0.004\n",
      "epoch (5): Train accuracy: 0.9596, loss: 0.004\n",
      "-----------Shadow model: 7-----------\n",
      "epoch (1): Train accuracy: 0.6668, loss: 0.031\n",
      "epoch (2): Train accuracy: 0.9000, loss: 0.010\n",
      "epoch (3): Train accuracy: 0.9272, loss: 0.007\n",
      "epoch (4): Train accuracy: 0.9480, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9532, loss: 0.004\n",
      "-----------Shadow model: 8-----------\n",
      "epoch (1): Train accuracy: 0.7428, loss: 0.025\n",
      "epoch (2): Train accuracy: 0.9296, loss: 0.007\n",
      "epoch (3): Train accuracy: 0.9568, loss: 0.004\n",
      "epoch (4): Train accuracy: 0.9652, loss: 0.003\n",
      "epoch (5): Train accuracy: 0.9740, loss: 0.003\n",
      "-----------Shadow model: 9-----------\n",
      "epoch (1): Train accuracy: 0.7424, loss: 0.025\n",
      "epoch (2): Train accuracy: 0.9292, loss: 0.008\n",
      "epoch (3): Train accuracy: 0.9520, loss: 0.005\n",
      "epoch (4): Train accuracy: 0.9460, loss: 0.005\n",
      "epoch (5): Train accuracy: 0.9708, loss: 0.003\n",
      "-----------Shadow model: 10-----------\n",
      "epoch (1): Train accuracy: 0.7656, loss: 0.022\n",
      "epoch (2): Train accuracy: 0.9328, loss: 0.007\n",
      "epoch (3): Train accuracy: 0.9560, loss: 0.004\n",
      "epoch (4): Train accuracy: 0.9584, loss: 0.004\n",
      "epoch (5): Train accuracy: 0.9756, loss: 0.002\n",
      "torch.Size([25000, 10])\n",
      "torch.Size([25000, 10])\n"
     ]
    }
   ],
   "source": [
    "#Pre-define a function to obtain member_set and non_member_set for a given shadow model\n",
    "def make_member_nonmember(shadow_model, shadow_loader, unseen_loader):   \n",
    "    shadow_model.eval()\n",
    "    \n",
    "    member_set, non_member_set = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in shadow_loader:\n",
    "            outputs = shadow_model(features)\n",
    "            probs = F.softmax(outputs, dim=1).type(torch.FloatTensor)\n",
    "            member_set.append(probs.detach())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in unseen_loader:\n",
    "            outputs = shadow_model(features)\n",
    "            probs = F.softmax(outputs, dim=1).type(torch.FloatTensor)\n",
    "            non_member_set.append(probs.detach())\n",
    "    \n",
    "    member_set = torch.cat(member_set)\n",
    "    non_member_set = torch.cat(non_member_set)\n",
    "    \n",
    "    return member_set, non_member_set\n",
    "\n",
    "    \n",
    "total_members = []\n",
    "total_non_members = []\n",
    "\n",
    "#Train a set of shadow models one by one\n",
    "for shadow_number, shadow_loader in enumerate(list_shadow_loader):\n",
    "    print(\"-----------Shadow model: {}-----------\".format(shadow_number + 1))\n",
    "    unseen_loader = list_unseen_loader[shadow_number]\n",
    "    \n",
    "    #Shadow model setting\n",
    "    shadow_model = CNN()\n",
    "    shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    #Train the target model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch, shadow_model, shadow_loader, shadow_optimizer, lossfunction)\n",
    "    \n",
    "    #Create member dataset vs non-member dataset based on shadow_model\n",
    "    member_set, non_member_set = make_member_nonmember(shadow_model, shadow_loader, unseen_loader)\n",
    "    \n",
    "    total_members.append(member_set)\n",
    "    total_non_members.append(non_member_set)\n",
    "\n",
    "total_members = torch.cat(total_members)\n",
    "total_non_members = torch.cat(total_non_members)\n",
    "\n",
    "print(total_members.shape)\n",
    "print(total_non_members.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5503666",
   "metadata": {},
   "source": [
    "####  Construct the final dataset for membership inference attack training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c7f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate total_members and total_non_members\n",
    "total_members_size = total_members.shape[0]\n",
    "total_non_members_size = total_non_members.shape[0]\n",
    "\n",
    "#Generate labels: member - 1 and non-member - 0\n",
    "total_members_labels = torch.Tensor([1]).repeat(total_members_size)\n",
    "total_non_members_labels = torch.Tensor([0]).repeat(total_non_members_size)\n",
    "\n",
    "#Final data for attack training\n",
    "X = torch.cat((total_members, total_non_members)).numpy()\n",
    "y = torch.cat((total_members_labels, total_non_members_labels)).numpy()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54628b6",
   "metadata": {},
   "source": [
    "## Train a binary classification model for membership inference attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4cc11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5105, F1_score: 0.5104\n"
     ]
    }
   ],
   "source": [
    "#Data splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#Attack model training using Logistic Regression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy: %.4f, F1_score: %.4f\" % (accuracy, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e03082",
   "metadata": {},
   "source": [
    "## Use the trained attack model to perform membership inference attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7a53c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5280, F1_score: 0.5279\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "test_size = 1000\n",
    "\n",
    "member_indices = np.random.choice(len(dataset_for_target), test_size, replace=False)\n",
    "non_member_indices = np.random.choice(len(dataset_for_shadow), test_size, replace=False)\n",
    "\n",
    "member_test = Subset(dataset_for_target, member_indices)\n",
    "non_member_test = Subset(dataset_for_shadow, non_member_indices)\n",
    "\n",
    "member_test_loader = DataLoader(member_test, batch_size=32, shuffle=False)\n",
    "non_member_test_loader = DataLoader(non_member_test, batch_size=32, shuffle=False)\n",
    "\n",
    "#Pass the data to the target model and obtain confidence scores\n",
    "member_test, non_member_test = make_member_nonmember(target_model, member_test_loader, non_member_test_loader)\n",
    "\n",
    "#Generate labels: member - 1 and non-member - 0\n",
    "member_test_labels = torch.Tensor([1]).repeat(test_size)\n",
    "non_member_test_labels = torch.Tensor([0]).repeat(test_size)\n",
    "\n",
    "#Final data for attack testing\n",
    "X_membership = torch.cat((member_test, non_member_test)).numpy()\n",
    "y_membership = torch.cat((member_test_labels, non_member_test_labels)).numpy()\n",
    "\n",
    "#Test the membership inference attack model \n",
    "y_pred_membership = log_reg.predict(X_membership)\n",
    "\n",
    "accuracy = accuracy_score(y_membership, y_pred_membership)\n",
    "f1 = f1_score(y_membership, y_pred_membership, average='macro')\n",
    "\n",
    "print(\"Accuracy: %.4f, F1_score: %.4f\" % (accuracy, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
