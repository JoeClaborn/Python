{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae721ee",
   "metadata": {},
   "source": [
    "# Simple feature-space adversarial attack\n",
    "\n",
    "We use the iris dataset to train a multi-class classification model, and then perform the simple feature-space adversarial attack on this model\n",
    "\n",
    "0: setosa, 1: versicolor, and 2: versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b47471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all the libraries here\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f316a0c",
   "metadata": {},
   "source": [
    "## Load iris dataset from scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31221457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Classe names***********\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "***********Feature names***********\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "***********Data sample festure values***********\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "***********Data size and feature size***********\n",
      "(150, 4)\n",
      "***********Data label values***********\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "#Classe names\n",
    "print(\"***********Classe names***********\")\n",
    "print(iris.target_names)\n",
    "#Feature names\n",
    "print(\"***********Feature names***********\")\n",
    "print(iris.feature_names)\n",
    "#Data sample festure values\n",
    "print(\"***********Data sample festure values***********\")\n",
    "print(iris.data)\n",
    "print(\"***********Data size and feature size***********\")\n",
    "print(iris.data.shape)\n",
    "#Data label values\n",
    "print(\"***********Data label values***********\")\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37de54",
   "metadata": {},
   "source": [
    "## Convert the data to PyTorch tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e6ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "#Split the data into two sets: 75% for training and 25% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset for training and testing, respectively\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders: here, we use mini-batch gradient descent, so need to specify the batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91859f16",
   "metadata": {},
   "source": [
    "## Construct a multi-class classification model (a softmax regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f24ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassification, self).__init__()\n",
    "        #The first \"4\" specifies that the feature dimension is 4, and the second \"3\" specifies that this is 3-class classification\n",
    "        #bias=False indicates that the output is calculated as y=w.x instead of y=w.x + b\n",
    "        self.fc = nn.Linear(4, 3, bias=False) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92c15b",
   "metadata": {},
   "source": [
    "## Set up some hyperparameters: use cross entropy loss, gradient descent with Adam optimizer, learning rate, and epochs; Pre-define the functions for training, testing, and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435a2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lossfunction = nn.CrossEntropyLoss() #Cross entropy loss for multi-class classification\n",
    "\n",
    "#Instantiate the model from \"MultiClassification\" class definition\n",
    "model = MultiClassification()\n",
    "\n",
    "#Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#Define the training function\n",
    "def train(epoch, model, train_dataloader, optimizer, lossfunction):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_total, train_correct = 0.0, 0.0 \n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the predicted output\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = lossfunction(predictions, y_batch)\n",
    "        \n",
    "        #Update the weights usning gradient descent with Adam optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Convert probabilities to multi-class predictions (reutrn the class with the maximal proability)\n",
    "        _, train_predicted = torch.max(predictions.data, 1)\n",
    "        \n",
    "        #Calculate the training statistics\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (train_predicted == y_batch).sum().item()\n",
    "\n",
    "    print(\"epoch (%d): Train accuracy: %.4f, loss: %.3f\" % (epoch, train_correct/train_total, train_loss/train_total))\n",
    "\n",
    "#Define the test function for test_dataloader\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct, test_total = 0.0, 0.0\n",
    "    y_test, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            _, test_predicted = torch.max(predictions.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (test_predicted == y_batch).sum().item()\n",
    "            \n",
    "            y_test += y_batch.tolist()\n",
    "            y_pred += test_predicted.tolist()\n",
    "\n",
    "    print('Test accuracy: %.4f' % (test_correct / test_total))\n",
    "    \n",
    "    return y_test, y_pred\n",
    "\n",
    "#Define the function that returns a predicted label for a single input sample\n",
    "def predict_label(model, single_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        prediction = model(single_input)\n",
    "        _, predicted_label = torch.max(prediction.data, 1)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "#Define the function that returns predicted probabilities for a single input sample\n",
    "def predict_probabilities(model, single_input):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        prediction = model(single_input)\n",
    "        predicted_probabilities = torch.softmax(prediction, dim=1).squeeze(0)\n",
    "    \n",
    "    return predicted_probabilities\n",
    "\n",
    "#Define the function that returns model weight vector that is used to predict the target_label\n",
    "def weight_vector(model, target_label):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights = list(model.parameters())[0]\n",
    "    \n",
    "    return weights[target_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2ca67",
   "metadata": {},
   "source": [
    "## Train the model using train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29f9223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1): Train accuracy: 0.6429, loss: 0.114\n",
      "epoch (2): Train accuracy: 0.6518, loss: 0.079\n",
      "epoch (3): Train accuracy: 0.6607, loss: 0.056\n",
      "epoch (4): Train accuracy: 0.4911, loss: 0.052\n",
      "epoch (5): Train accuracy: 0.4554, loss: 0.051\n",
      "epoch (6): Train accuracy: 0.7411, loss: 0.047\n",
      "epoch (7): Train accuracy: 0.6696, loss: 0.045\n",
      "epoch (8): Train accuracy: 0.6786, loss: 0.043\n",
      "epoch (9): Train accuracy: 0.7143, loss: 0.042\n",
      "epoch (10): Train accuracy: 0.7411, loss: 0.041\n",
      "epoch (11): Train accuracy: 0.7411, loss: 0.039\n",
      "epoch (12): Train accuracy: 0.7321, loss: 0.038\n",
      "epoch (13): Train accuracy: 0.7768, loss: 0.037\n",
      "epoch (14): Train accuracy: 0.7857, loss: 0.036\n",
      "epoch (15): Train accuracy: 0.8125, loss: 0.036\n",
      "epoch (16): Train accuracy: 0.8214, loss: 0.035\n",
      "epoch (17): Train accuracy: 0.7768, loss: 0.034\n",
      "epoch (18): Train accuracy: 0.8393, loss: 0.034\n",
      "epoch (19): Train accuracy: 0.8661, loss: 0.033\n",
      "epoch (20): Train accuracy: 0.8304, loss: 0.032\n",
      "epoch (21): Train accuracy: 0.8571, loss: 0.032\n",
      "epoch (22): Train accuracy: 0.9107, loss: 0.031\n",
      "epoch (23): Train accuracy: 0.8661, loss: 0.031\n",
      "epoch (24): Train accuracy: 0.8125, loss: 0.030\n",
      "epoch (25): Train accuracy: 0.8929, loss: 0.030\n",
      "epoch (26): Train accuracy: 0.9196, loss: 0.029\n",
      "epoch (27): Train accuracy: 0.9107, loss: 0.029\n",
      "epoch (28): Train accuracy: 0.9018, loss: 0.029\n",
      "epoch (29): Train accuracy: 0.9286, loss: 0.028\n",
      "epoch (30): Train accuracy: 0.9554, loss: 0.028\n",
      "epoch (31): Train accuracy: 0.8304, loss: 0.028\n",
      "epoch (32): Train accuracy: 0.8839, loss: 0.027\n",
      "epoch (33): Train accuracy: 0.9464, loss: 0.027\n",
      "epoch (34): Train accuracy: 0.9643, loss: 0.027\n",
      "epoch (35): Train accuracy: 0.9107, loss: 0.027\n",
      "epoch (36): Train accuracy: 0.8571, loss: 0.026\n",
      "epoch (37): Train accuracy: 0.9732, loss: 0.026\n",
      "epoch (38): Train accuracy: 0.9464, loss: 0.026\n",
      "epoch (39): Train accuracy: 0.9196, loss: 0.025\n",
      "epoch (40): Train accuracy: 0.9375, loss: 0.025\n",
      "epoch (41): Train accuracy: 0.9643, loss: 0.025\n",
      "epoch (42): Train accuracy: 0.9643, loss: 0.025\n",
      "epoch (43): Train accuracy: 0.9464, loss: 0.024\n",
      "epoch (44): Train accuracy: 0.9464, loss: 0.024\n",
      "epoch (45): Train accuracy: 0.9554, loss: 0.024\n",
      "epoch (46): Train accuracy: 0.9554, loss: 0.024\n",
      "epoch (47): Train accuracy: 0.9375, loss: 0.023\n",
      "epoch (48): Train accuracy: 0.9732, loss: 0.023\n",
      "epoch (49): Train accuracy: 0.9821, loss: 0.023\n",
      "epoch (50): Train accuracy: 0.9643, loss: 0.023\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, model, train_dataloader, optimizer, lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d304d",
   "metadata": {},
   "source": [
    "## Implement a simple feature-space adversarial attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9557e7",
   "metadata": {},
   "source": [
    "#### Randomly select an test input from test_dataset to perturb and select a target label to attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16f962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the test input:  28\n",
      "Test input feature vector:  tensor([4.8000, 3.0000, 1.4000, 0.3000])\n",
      "Test input original label:  0 setosa\n",
      "Target label:  2 virginica\n"
     ]
    }
   ],
   "source": [
    "#Set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Number of test samples\n",
    "number_of_samples = len(test_dataset)\n",
    "#Get a random index from [0, number_of_samples)\n",
    "index = np.random.randint(number_of_samples)\n",
    "\n",
    "#Select the test input to perturb\n",
    "test_input = test_dataset[index][0]\n",
    "test_input_label = test_dataset[index][1]\n",
    "\n",
    "#Here, we perform targeted adversarial attack: change the original_label to the target_label\n",
    "original_label = test_input_label.item() #0-Setosa in this example\n",
    "target_label = 2 #2-Virginica in this example\n",
    "\n",
    "print(\"The index of the test input: \", index)\n",
    "print(\"Test input feature vector: \", test_input)\n",
    "print(\"Test input original label: \", original_label, iris.target_names[original_label])\n",
    "print(\"Target label: \", target_label, iris.target_names[target_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c8a47",
   "metadata": {},
   "source": [
    "#### Search for a good instance from test_dataset for guidance\n",
    "\n",
    "Find those test samples that are predicted as the target label and also closest to decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6463a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.7000, 2.6000, 6.9000, 2.3000],\n",
      "        [6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000],\n",
      "        [6.5000, 3.2000, 5.1000, 2.0000],\n",
      "        [6.5000, 3.0000, 5.8000, 2.2000],\n",
      "        [6.4000, 2.8000, 5.6000, 2.2000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [6.4000, 2.8000, 5.6000, 2.1000],\n",
      "        [7.9000, 3.8000, 6.4000, 2.0000],\n",
      "        [6.7000, 3.0000, 5.2000, 2.3000],\n",
      "        [6.7000, 2.5000, 5.8000, 1.8000],\n",
      "        [6.8000, 3.2000, 5.9000, 2.3000],\n",
      "        [6.3000, 2.5000, 5.0000, 1.9000]])\n",
      "tensor([[1.1105e-04, 1.0091e-01, 8.9898e-01],\n",
      "        [5.8958e-03, 3.4968e-01, 6.4442e-01],\n",
      "        [9.5101e-03, 3.6839e-01, 6.2210e-01],\n",
      "        [9.7916e-03, 4.1667e-01, 5.7353e-01],\n",
      "        [1.3993e-03, 2.2372e-01, 7.7488e-01],\n",
      "        [1.3151e-03, 2.0696e-01, 7.9172e-01],\n",
      "        [1.1523e-02, 4.1935e-01, 5.6913e-01],\n",
      "        [1.5738e-03, 2.2441e-01, 7.7402e-01],\n",
      "        [5.0586e-03, 4.5233e-01, 5.4261e-01],\n",
      "        [3.8024e-03, 2.9794e-01, 6.9826e-01],\n",
      "        [1.1834e-03, 2.1460e-01, 7.8422e-01],\n",
      "        [1.6284e-03, 2.4706e-01, 7.5131e-01],\n",
      "        [3.6170e-03, 2.7816e-01, 7.1823e-01]])\n",
      "tensor([6, 3, 2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "target_samples = [] #Test samples that are predicted as the target label\n",
    "target_probs = []   #The prediction probabilities for each test sample\n",
    "\n",
    "for sample, true_label in test_dataset:\n",
    "    predicted_label = predict_label(model, sample.unsqueeze(0)) #unsqueeze(0) is used to ensure batch dimension\n",
    "    predicted_proabilities = predict_probabilities(model, sample.unsqueeze(0))\n",
    "\n",
    "    if predicted_label == target_label:\n",
    "        target_samples.append(sample)\n",
    "        target_probs.append(predicted_proabilities)\n",
    "\n",
    "target_samples = torch.stack(target_samples)\n",
    "target_probs = torch.stack(target_probs)\n",
    "\n",
    "print(target_samples)\n",
    "print(target_probs)\n",
    "\n",
    "#Target samples that are closest to the decision boundary should be those that have the highest probability for the original label\n",
    "#Rank target samples by highest probability for the original label\n",
    "closest_to_boundary_indices = torch.argsort(target_probs[:, original_label], descending=True)\n",
    "\n",
    "k = 5\n",
    "top_k_boundary_indices = closest_to_boundary_indices[:k]\n",
    "print(top_k_boundary_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67c4bf",
   "metadata": {},
   "source": [
    "Calculate the manhattan distance between test_input and target_samples and find those target_samples that are closest to test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eade1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.8000,  7.9000,  6.5000,  7.3000,  8.0000,  7.9000,  6.3000,  7.8000,\n",
      "        10.6000,  7.7000,  8.3000,  8.7000,  7.2000])\n",
      "tensor([ 6,  2, 12,  3,  9,  7,  1,  5,  4, 10, 11,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "#Calculate manhattan distance (L1 distance) between test_input and target_samples\n",
    "distances = torch.sum(torch.abs(test_input - target_samples), dim=1)\n",
    "print(distances)\n",
    "\n",
    "#Target samples that are closest to the test_input should be those that have the shortest distance to the test_input\n",
    "#Rank target samples by shortest distance to the test_input\n",
    "nearest_neighbors_indices = torch.argsort(distances)\n",
    "print(nearest_neighbors_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58b4f5",
   "metadata": {},
   "source": [
    "Find a good instance for guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d07b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The found good instance is:  tensor([6.1000, 3.0000, 4.9000, 1.8000])\n"
     ]
    }
   ],
   "source": [
    "#The good instance is initialized as the nearest neighbor\n",
    "good_instance = target_samples[nearest_neighbors_indices[0]]\n",
    "\n",
    "#A good instance is one of the test_input's nearest neighbors that is among top k target samples close to decision boundary\n",
    "for i in nearest_neighbors_indices:\n",
    "    if i in top_k_boundary_indices:\n",
    "        good_instance = target_samples[i]\n",
    "        break\n",
    "\n",
    "print(\"The found good instance is: \", good_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e8517",
   "metadata": {},
   "source": [
    "#### Find a feature to perturb\n",
    "\n",
    "Once we find a good instance for guidance, we can use greedy search to perturb individual feature from the most important one to the least important one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038a4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ordered by importance:  ['petal width (cm)', 'petal length (cm)', 'sepal length (cm)', 'sepal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#Feature importance for the target label can be quantified by the weight vector to predict target label\n",
    "featue_importances = weight_vector(model, target_label)\n",
    "\n",
    "#Find the indices of features from the most important to the least important \n",
    "featue_importances_indices = torch.argsort(featue_importances, descending=True)\n",
    "\n",
    "print(\"Features ordered by importance: \", [iris.feature_names[i] for i in featue_importances_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7051a29",
   "metadata": {},
   "source": [
    "#### Perform the perturbation to generate an adversarial example\n",
    "\n",
    "There are two ways here: <br>\n",
    "(1) Directly update the value of the specific feature in the original test input to the value of that in the good instance <br>\n",
    "(2) When perturbing a feature, fine-tune the perturbation by adding the feature value step by step\n",
    "\n",
    "Here, let's look at the first way first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e867792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial attack succeeds!\n",
      "The original test input is:  tensor([4.8000, 3.0000, 1.4000, 0.3000])\n",
      "The target instance is:  tensor([6.1000, 3.0000, 4.9000, 1.8000])\n",
      "The adversarial example is:  tensor([4.8000, 3.0000, 4.9000, 1.8000])\n",
      "The size of perturbation is: 5.0\n"
     ]
    }
   ],
   "source": [
    "adversarial_example = test_input.clone()\n",
    "for feature in featue_importances_indices:\n",
    "    adversarial_example[feature] = good_instance[feature]\n",
    "    if predict_label(model, adversarial_example.unsqueeze(0)) == target_label:\n",
    "        break\n",
    "        \n",
    "perturbation_size = torch.sum(torch.abs(adversarial_example - test_input))\n",
    "\n",
    "print(\"Adversarial attack succeeds!\")\n",
    "print(\"The original test input is: \", test_input)\n",
    "print(\"The target instance is: \", good_instance)\n",
    "print(\"The adversarial example is: \", adversarial_example)\n",
    "print(\"The size of perturbation is: {:.1f}\".format(perturbation_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda42a21",
   "metadata": {},
   "source": [
    "Then, let's look at the second way to fine-tune the perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0265989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial attack succeeds!\n",
      "The original input is:  tensor([4.8000, 3.0000, 1.4000, 0.3000])\n",
      "The target instance is:  tensor([6.1000, 3.0000, 4.9000, 1.8000])\n",
      "The adversarial example is:  tensor([4.8000, 3.0000, 4.0000, 1.8000])\n",
      "The size of perturbation is: 4.1\n"
     ]
    }
   ],
   "source": [
    "adversarial_example = test_input.clone()\n",
    "step_size = 0.1\n",
    "success_flag = 0\n",
    "\n",
    "for feature in featue_importances_indices:\n",
    "    original_val = adversarial_example[feature]\n",
    "    target_val = good_instance[feature]\n",
    "    \n",
    "    #Perturb the feature value towards the good_instance's value\n",
    "    if original_val < target_val:\n",
    "        while adversarial_example[feature] < target_val:\n",
    "            adversarial_example[feature] += step_size\n",
    "            #Ensure it doesn't go beyond the value in good_instance\n",
    "            if adversarial_example[feature] > target_val:\n",
    "                adversarial_example[feature] = target_val\n",
    "            #Check if the model predicts the target label\n",
    "            if predict_label(model, adversarial_example.unsqueeze(0)) == target_label:\n",
    "                success_flag = 1\n",
    "                break\n",
    "    \n",
    "    elif original_val > target_val:\n",
    "        while adversarial_example[feature] > target_val:\n",
    "            adversarial_example[feature] -= step_size\n",
    "            #Ensure it doesn't go beyond the value in good_instance\n",
    "            if adversarial_example[feature] < target_val:\n",
    "                adversarial_example[feature] = target_val\n",
    "            #Check if the model predicts the target label\n",
    "            if predict_label(model, adversarial_example.unsqueeze(0)) == target_label:\n",
    "                success_flag = 1\n",
    "                break   \n",
    "    if success_flag:\n",
    "        break\n",
    "\n",
    "perturbation_size = torch.sum(torch.abs(adversarial_example - test_input))\n",
    "\n",
    "print(\"Adversarial attack succeeds!\")\n",
    "print(\"The original input is: \", test_input)\n",
    "print(\"The target instance is: \", good_instance)\n",
    "print(\"The adversarial example is: \", adversarial_example)\n",
    "print(\"The size of perturbation is: {:.1f}\".format(perturbation_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
